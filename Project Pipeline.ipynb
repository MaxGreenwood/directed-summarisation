{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Project Pipeline.ipynb","provenance":[{"file_id":"1XcQOc7m-oumeX96o5SA5EoMhuQrQtxJh","timestamp":1661703039500},{"file_id":"1jt9z7fKLPWFM31gRjKHTiPhqwRu8oVsN","timestamp":1658339364948},{"file_id":"1PsWQJvkz9mvvJyv1bxDv2dRecN7Tm7G1","timestamp":1658228796344}],"collapsed_sections":["PlxXWl3HE_tW","DFD7xbeKQbkI"],"machine_shape":"hm","toc_visible":true,"mount_file_id":"1j9Bj4km5HIGM7nfUxjRZo0Cq9-cAtQjc","authorship_tag":"ABX9TyM6V6dQl/jDdYfWNW2gntMn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# I. Load datasets and import libraries"],"metadata":{"id":"fdTOhPLl-VU5"}},{"cell_type":"code","source":["# Mount Google Drive\n","import os, sys\n","from IPython.display import clear_output\n","# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"HOqDewNoLhc2","executionInfo":{"status":"ok","timestamp":1661703508208,"user_tz":-60,"elapsed":233,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["! pip install evaluate\n","! pip install sacrebleu\n","! pip install -r rouge/requirements.txt\n","! pip install rouge-score\n","from rouge_score import rouge_scorer\n","import evaluate\n","clear_output()"],"metadata":{"id":"eDM4r9scSndP","executionInfo":{"status":"ok","timestamp":1661705175958,"user_tz":-60,"elapsed":10737,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Load amrlib library from drive\n","amrlib_path = '/content/amrlib'\n","os.symlink('/content/drive/MyDrive/IP/amrlib', amrlib_path)\n","sys.path.insert(0, amrlib_path)\n","\n","# Install and import dependencies to environment\n","! pip install --target=$amrlib_path jdc\n","os.chdir('/content/amrlib')\n","! pip install -r requirements.txt\n","\n","import amrlib\n","import penman\n","import transformers\n","import unidecode\n","import word2number\n","import sentencepiece\n","\n","clear_output()"],"metadata":{"id":"lCZjqjHb49bP","executionInfo":{"status":"ok","timestamp":1661705214948,"user_tz":-60,"elapsed":39001,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Import all other necessary libraries\n","import seaborn as sns\n","sns.set_theme()\n","import pandas as pd\n","import numpy as np\n","import torch\n","import requests\n","import tqdm\n","import time\n","import urllib.parse\n","import json\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import random\n","import tarfile\n","import csv\n","from ast import literal_eval\n","import pickle\n","import torch\n","\n","# Preprocessing\n","import nltk\n","import pprint\n","import re\n","from nltk.tokenize.api import TokenizerI\n","from nltk.tokenize.util import regexp_span_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('omw-1.4')\n","\n","clear_output()"],"metadata":{"id":"RgdIIv5bBqfY","executionInfo":{"status":"ok","timestamp":1661705214950,"user_tz":-60,"elapsed":23,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### Choose data input: AMR Proxy or CNNDM"],"metadata":{"id":"E4a_M3-AdUna"}},{"cell_type":"markdown","source":["Each data folder should contain .txt files of the AMRs (with annotations), tokenised text and non-tokenised document text. They should be labelled `amr.txt`, `tok_stories.txt` and `snt_stories.txt` respectively. The story text can be obtained by running `pipeline.py` of the Unsupervised SAS program up to and including the `save_stories()` function, with the `amr.txt` file as input.\n","\n"],"metadata":{"id":"Ca4tsYaPUqKs"}},{"cell_type":"code","source":["# Uncomment dataset to process\n","# dataset = 'amr_proxy'\n","# dataset = 'cnndm'\n","# dataset = 'test'\n","dataset = 'moral_test'\n","data_path = f'/content/drive/MyDrive/IP/datasets/{dataset}_inputs'\n","output_path = f'/content/drive/MyDrive/IP/datasets/{dataset}_outputs'"],"metadata":{"id":"nkNbHjJYdalU","executionInfo":{"status":"ok","timestamp":1661705214951,"user_tz":-60,"elapsed":20,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Extract AMR bank files from tgz file in drive\n","filename = '/content/drive/MyDrive/IP/amr_annotation_3.0_LDC2020T02.tgz'\n","if filename.endswith(\"tgz\"):\n","    tar = tarfile.open(filename, \"r:gz\")\n","    for item in tar:\n","        tar.extract(item, '/content')\n","    tar.close()"],"metadata":{"id":"5Not3kPrKVkT","executionInfo":{"status":"ok","timestamp":1661705218364,"user_tz":-60,"elapsed":3432,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["*End of setup*\n","\n","--------------------------------------------------------------------------------"],"metadata":{"id":"3MzXUJyKyhi0"}},{"cell_type":"markdown","source":["### Load datasets"],"metadata":{"id":"eKcDUst42poP"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"xXzBYTXEVnuL","executionInfo":{"status":"ok","timestamp":1661705784870,"user_tz":-60,"elapsed":189468,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}}},"outputs":[],"source":["# Load CNNDM dataset from HuggingFace\n","! pip install datasets\n","from datasets import load_dataset\n","cnndm_data = load_dataset(\"cnn_dailymail\",\"3.0.0\")\n","clear_output()"]},{"cell_type":"code","source":["print('Training articles =', len(cnndm_data['train']))\n","print('Validation articles =', len(cnndm_data['validation']))\n","print('Testing articles =', len(cnndm_data['test']))"],"metadata":{"id":"ZMrxcKrZC3d8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661705784871,"user_tz":-60,"elapsed":15,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"0250a5b1-b591-4486-a9c2-c703e0461551"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Training articles = 287113\n","Validation articles = 13368\n","Testing articles = 11490\n"]}]},{"cell_type":"markdown","source":["### Filter datasets for opinion articles"],"metadata":{"id":"ie6CaGzS-eUX"}},{"cell_type":"code","source":["# For CNNDM dataset, use keyphrase 'opinions expressed' to identify opinion pieces.\n","def filter_opinions(dataset):\n","    start_time = time.time()\n","    opinions = dataset.filter(lambda x: 'opinions expressed in this' in x['article'])\n","    clear_output()\n","    print('Finished in %s seconds' % (time.time() - start_time))\n","    print('New corpus size =', len(opinions))\n","    return opinions"],"metadata":{"id":"_dh78ZF8X0u5","executionInfo":{"status":"ok","timestamp":1661708877097,"user_tz":-60,"elapsed":223,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["corpus = filter_opinions(cnndm_data['train'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WL6ShDh2BiwM","executionInfo":{"status":"ok","timestamp":1661708878643,"user_tz":-60,"elapsed":219,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"1ae749af-2518-451a-c652-325d360d122e"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished in 0.008241415023803711 seconds\n","New corpus size = 4898\n"]}]},{"cell_type":"markdown","source":["### Preprocess articles"],"metadata":{"id":"j2boGr9fc5cq"}},{"cell_type":"code","source":["def preprocess(article):\n","  \n","    # Split into list of sentences\n","    sent_list = nltk.tokenize.sent_tokenize(article)\n","\n","    # Remove extraneous info from start of article\n","    for i, sent in enumerate(sent_list):\n","\n","        if \"(CNN) --\" in sent:\n","            index = sent.find('(CNN) --')\n","            new_sent = \"\"\n","            new_sent = [new_sent+char for idx, char in enumerate(sent_list[i]) if idx > index+8]\n","            new_sent = ''.join((str(n) for n in new_sent))\n","            sent_list[i] = new_sent\n","            sent_list = sent_list[i:-1]\n","\n","    return sent_list"],"metadata":{"id":"nTynZ-4Rdf6z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Stage 1: Text-to-AMR parsing (for CNNDM)"],"metadata":{"id":"CtJZGhfA4-OB"}},{"cell_type":"markdown","source":["https://github.com/bjascob/amrlib\n","\n","* Input: corpus of articles (HuggingFace dataset) ---\n","* Intermediate: table of article AMRs, sentences and graphs (.csv file)\n","* Output: AMR Proxy text format (.txt file)"],"metadata":{"id":"I9maYicj_0nF"}},{"cell_type":"markdown","source":["### Functions"],"metadata":{"id":"75I_E30bC9SR"}},{"cell_type":"code","source":["def process_test_corpus(corpus, csv_file):\n","  \n","    with open('/content/drive/MyDrive/IP/datasets/test_selection.csv', 'r', encoding='latin-1') as file:\n","        csv_reader = csv.reader(file, quotechar = '\"')\n","        ids = []\n","        moral_summs = []\n","        for idx, line in enumerate(csv_reader):\n","            if idx > 0:\n","                ids.append(line[4])\n","                moral_summs.append(line[5])\n","\n","    start_time = time.time()\n","    num_records = len(ids)\n","\n","    total = num_records\n","    with tqdm.tqdm(total=total) as bar:\n","\n","        with open(csv_file, 'r+', newline='') as file:\n","            reader = csv.DictReader(file)\n","            fieldnames = ['article', 'highlights', 'id', 'article_sentences', 'article_penmans', 'summary_sentences', 'summary_penmans', 'moral_sentences', 'moral_penmans']\n","            writer = csv.DictWriter(file, fieldnames=fieldnames)\n","\n","            for idx, id in enumerate(ids):\n","                for art in corpus:\n","                    if art['id'] == id:\n","\n","                        # Preprocess the article\n","                        article_sents = preprocess(art['article'])\n","                        # Parse each artcile sentence to an AMR\n","                        article_penmans = parser.parse_sents(article_sents)\n","                        # Preprocess the highlights\n","                        summary_sents = preprocess(art['highlights'])\n","                        # Parse each summary sentence to an AMR\n","                        summary_penmans = parser.parse_sents(summary_sents)\n","                        # Preprocess moral summary sentences\n","                        moral_sents = preprocess(moral_summs[idx])\n","                        # Parse each moral summary sentence to an AMR\n","                        moral_penmans = parser.parse_sents(moral_sents)\n","                        \n","                        writer.writerow({'article': art['article'], \n","                                        'highlights': art['highlights'], \n","                                        'id': art['id'], \n","                                        'article_sentences': article_sents, \n","                                        'article_penmans': article_penmans, \n","                                        'summary_sentences': summary_sents, \n","                                        'summary_penmans': summary_penmans,\n","                                         'moral_sentences': moral_sents,\n","                                         'moral_penmans': moral_penmans})   \n","                torch.cuda.empty_cache()\n","                bar.update(1)"],"metadata":{"id":"rUVSJ24MCB2i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Parse articles"],"metadata":{"id":"we0b7UhM6xCZ"}},{"cell_type":"code","source":["# Instantiate parser: 'parse_xfm_bart_large' model\n","parser = amrlib.load_stog_model(model_dir='/content/drive/MyDrive/IP/amrlib/data/model_stog')\n","clear_output()"],"metadata":{"id":"kIxWzUTOlUE6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Process articles to csv file\n","process_test_corpus(corpus, csv_file='/content/drive/MyDrive/IP/datasets/processed_test.csv')"],"metadata":{"id":"2SNIaHrokzsP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load previously processed dataset"],"metadata":{"id":"4FBvNdY065nW"}},{"cell_type":"code","source":["def load_dataset(filepath):\n","    corpus = pd.read_csv(filepath)\n","    corpus.name = filepath.split('/')[-1][:-4] # Give name\n","    return corpus"],"metadata":{"id":"EuuBtYZv9eJt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load from datasets folder in drive\n","test_corpus = load_dataset('/content/drive/MyDrive/IP/datasets/processed_test.csv')\n","test_corpus.name = 'test_corpus'"],"metadata":{"id":"3gw71xKZ7G5m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Aligments"],"metadata":{"id":"UPL-O0yyInNi"}},{"cell_type":"markdown","source":["https://github.com/clab/fast_align\n","* Input: tokenised sentences and PENMANs (from .csv file)\n","* Ouput: list of alignments (written into .txt file)"],"metadata":{"id":"jSf43zJWVRjT"}},{"cell_type":"code","source":["# Compile C++ code and build binaries\n","from os.path import exists\n","path = '/content/drive/MyDrive/IP/amrlib/alignments/faa_aligner/fast_align-master'\n","os.chdir(path)\n","if exists(f\"{path}/fast_align\"):\n","    ! rm \"{path}/fast_align\"\n","    ! rm \"{path}/atools\"\n","    ! mkdir build\n","    ! cd build\n","    ! cmake .\n","    ! make\n","clear_output()"],"metadata":{"id":"romN-9lEYuyE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set environment variable to location of binaries\n","os.environ['FABIN_DIR'] = '/content/drive/MyDrive/IP/amrlib/alignments/faa_aligner/fast_align-master'\n","from amrlib.alignments.faa_aligner import FAA_Aligner\n","from amrlib.evaluate.alignment_scorer import AlignmentScorer"],"metadata":{"id":"HySy-pzZHk6x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Write sentence AMRs to PENMAN .txt file for summary graph extraction"],"metadata":{"id":"7mfoKzco_MwW"}},{"cell_type":"code","source":["def amr_write(corpus, art_index, sent_index, penman, penman_number, output, aligner, key, snt_type):\n","    # Assign ID and sentence type\n","    output.write(f\"# ::id {corpus.iloc[art_index]['id'][:8]}.{str(penman_number)}\" + f\" ::snt-type {snt_type}\" + \"\\n\")\n","    # Assign tokens\n","    sentence = literal_eval(corpus.iloc[art_index][key])[sent_index]\n","    tokens = nltk.tokenize.word_tokenize(sentence)\n","    joined_tokens = \" \".join(tokens)\n","    output.write(f\"# ::tok {joined_tokens}\" + \"\\n\")\n","    # Create alignments\n","    amr_surface_aligns, alignment_strings = aligner.align_sents([joined_tokens], [penman])\n","    output.write(f\"# ::alignments {alignment_strings[0]}\" + \"\\n\")\n","    # Write PENMAN\n","    output.write(str(penman) + \"\\n\\n\")\n","\n","\n","def write_penman_doc(corpus, path):\n","    inference = FAA_Aligner() # Instantiate aligner object\n","    total = len(corpus)\n","\n","    # SOME ARTICLES ARE NOT SUCCESSFULLY ALIGNED\n","    # Identify these article and update unaligned articles\n","    unaligned_articles = [25]\n","\n","    with tqdm.tqdm(total=total) as bar:\n","        with open(f\"{path}/{corpus.name}.txt\", \"w\") as output:\n","            article_num = 1\n","            # Loop over articles\n","            output.write('CNNDM AMRs'+'\\n\\n')\n","            for i in range(len(corpus)):\n","                penman_num = 1\n","                output.write('# ::snt-type date [signifies start of new document]'+'\\n\\n')\n","                # Write summaries\n","                if i not in unaligned_articles:\n","                        for j, penman in enumerate(literal_eval(corpus.iloc[i]['moral_penmans'])):\n","                            try: \n","                                amr_write(corpus, i, j, penman, penman_num, output, inference, key='moral_sentences', snt_type='summary')\n","                                penman_num += 1\n","                            except AttributeError:\n","                                unaligned_articles.append(i)\n","                                # print(f'Failed to retrieve alignments for article {i}')\n","                                continue\n","                        # Write articles\n","                        for k, penman in enumerate(literal_eval(corpus.iloc[i]['article_penmans'])):\n","                            amr_write(corpus, i, k, penman, penman_num, output, inference, key='article_sentences', snt_type='body')\n","                            penman_num += 1\n","\n","                article_num += 1\n","                bar.update(1)\n","\n","    return unaligned_articles"],"metadata":{"id":"DQjWKxEXYV2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Write the document\n","unaligned_articles = write_penman_doc(test_corpus, data_path)"],"metadata":{"id":"pYQP0QcUs2JJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660234227515,"user_tz":-60,"elapsed":96311,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"6acc9e45-e521-4cf2-dc28-6bb9aeac3dd2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [01:36<00:00,  1.92s/it]\n"]}]},{"cell_type":"markdown","source":["### Measure alignments"],"metadata":{"id":"JhuY60n8mJKu"}},{"cell_type":"code","source":["inference = FAA_Aligner()\n","\n","with open ('/content/drive/MyDrive/IP/datasets/amr_proxy_dict.txt', 'rb') as data:\n","    amr_dict = pickle.load(data)\n","\n","    ref_toks = []\n","    ref_penmans = []\n","    ref_alignments = []\n","    ref_alignments_list = []\n","\n","    for doc in amr_dict:\n","        for sent in doc:\n","              ref_alignments.append(\" \".join(sent['alignments']))\n","              ref_alignments_list.append(sent['alignments'])\n","              ref_toks.append(\"\".join(sent['tok']))\n","              ref_penmans.append(\" \".join(sent['amr']))\n","\n","amr_surface_aligns, test_alignments = inference.align_sents(ref_toks, ref_penmans)\n","test_alignments_list = []\n","for alignment in test_aligments:\n","    test_alignments_list.append(alignment)\n","\n","scorer = AlignmentScorer(ref_alignments_list, test_alignments_list)\n","scores = scorer.get_precision_recall_f1()\n","print(scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dgtU3F-u7gGW","executionInfo":{"status":"ok","timestamp":1658615166321,"user_tz":-60,"elapsed":3177,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"f4b25b5a-479e-4326-a7bb-cce117a9cd5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(0.7438520351727903, 0.5604987354609473, 0.6392883486116707)\n"]}]},{"cell_type":"markdown","source":["# II: Create pipeline inputs"],"metadata":{"id":"t0x9M0dbLTgZ"}},{"cell_type":"markdown","source":["### a. Coreference resolution \n","\n","*(3-4 articles per min)*\n","\n","https://github.com/kentonl/e2e-coref\n","\n","* Input: tokenised stories (.txt file)\n","* Ouput: dataframe of sentences, predicted_clusters, top_spans, head_scores (.csv file)"],"metadata":{"id":"nGVLzhF9rVOC"}},{"cell_type":"code","source":["os.chdir('/content')\n","! git clone https://github.com/kentonl/e2e-coref\n","%cd e2e-coref\n","\n","# Temporary hack\n","! sed 's/MarkupSafe==1.0/MarkupSafe==1.1.1/; s/scikit-learn==0.19.1/scikit-learn==0.21/; s/scipy==1.0.0/scipy==1.6.2/' < requirements.txt > tmp\n","! mv tmp requirements.txt\n","! sed 's/.D.GLIBCXX.USE.CXX11.ABI.0//' < setup_all.sh  > tmp\n","! mv tmp setup_all.sh \n","! chmod u+x setup_all.sh \n","\n","# Set environment variables\n","os.environ['data_dir'] = \".\"\n","os.environ['TAR'] = '/content/drive/MyDrive/IP/e2e-coref/e2e-coref.tgz'\n","clear_output()"],"metadata":{"id":"WSL401ktrVOD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Copy edited python files and requirements from drive to Colab space\n","! cp '/content/drive/MyDrive/IP/e2e-coref/demo.py' '/content/e2e-coref'\n","! cp '/content/drive/MyDrive/IP/e2e-coref/requirements.txt' '/content/e2e-coref'\n","! cp '/content/drive/MyDrive/IP/e2e-coref/char_vocab.english.txt' '/content/e2e-coref'\n","! cp '/content/drive/MyDrive/IP/e2e-coref/resolver.py' '/content/e2e-coref'"],"metadata":{"id":"NPJSGGQqofQ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run setup from clean slate\n","! pip uninstall -y tensorflow\n","os.chdir('/content/drive/MyDrive/IP/e2e-coref/')\n","! pip install -r requirements.txt --log install-log.txt -q\n","! pip install scikit-learn==0.22.2 # Takes care of error\n","os.chdir('/content/e2e-coref/')\n","! ./setup_all.sh\n","! tar xvzf $TAR\n","clear_output()"],"metadata":{"id":"YmUreTTUrVOD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create coref input\n","with open(f'{data_path}/tok_stories.txt', 'r', encoding='latin-1') as file:\n","    stories = []\n","    for line in file:\n","       story = line\n","       sent_list = nltk.tokenize.sent_tokenize(story)\n","       stories.append(sent_list)\n","amr_proxy = pd.DataFrame()\n","amr_proxy['sentences'] = stories\n","amr_proxy.to_csv('/content/e2e-coref/coref_input.csv')\n"],"metadata":{"id":"tFB3q_MHBNJz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run the coreference resolution program\n","os.chdir('/content/e2e-coref')\n","! GPU=1 python resolver.py final\n","clear_output()"],"metadata":{"id":"T3kwqdyFrVOD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Copy result to data folder\n","! cp \"/content/e2e-coref/coref.csv\" {data_path}"],"metadata":{"id":"NCaGRZ00dCkW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### b. TF-IDF\n","\n","*(~800 articles per min)*\n","* Input: non-tokenised stories (.txt file)\n","* Output: dictionary of IDF scores over vocab (.json file)"],"metadata":{"id":"NGMbqiEXrz0e"}},{"cell_type":"code","source":["train_set = pd.DataFrame(cnndm_data['train'])"],"metadata":{"id":"i0gjxImCJHbV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating IDF scores for each word in the corpus\n","\n","df_dict = {}\n","idf_dict = {}\n","\n","# Add words from dataset if AMR Proxy\n","if dataset == 'amr_proxy':\n","    with open(f'{data_path}/snt_stories.txt', 'r', encoding='latin-1') as file:\n","        stories = file.readlines()\n","        for story in stories:\n","            unique_words = set()\n","            words = nltk.tokenize.word_tokenize(story)\n","            for word in words:\n","                if word.lower() not in stopwords.words('english'):\n","                    unique_words.add(word.lower())\n","                    if word.lower() not in df_dict:\n","                        df_dict[word.lower()] = 0\n","\n","            for word in list(unique_words):\n","                df_dict[word] += 1\n","\n","total = len(train_set)\n","with tqdm.tqdm(total=total) as bar:\n","\n","    for idx, article in enumerate(train_set['article']):\n","        sent_list = preprocess(article)\n","        unique_words = set()\n","        for sent in sent_list:\n","            words = nltk.tokenize.word_tokenize(sent)\n","            for word in words:\n","                if word.lower() not in stopwords.words('english'):\n","                    unique_words.add(word.lower())\n","                    if word.lower() not in df_dict:\n","                        df_dict[word.lower()] = 0\n","\n","        for word in list(unique_words):\n","            df_dict[word] += 1\n","        bar.update(1)\n","\n","# Using the equation given by the authors\n","for word in df_dict.keys():\n","    idf_dict[word] = np.log(len(train_set) / (df_dict[word] + 1))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-D4aVPYicFvq","executionInfo":{"status":"ok","timestamp":1659554702089,"user_tz":-60,"elapsed":22599588,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"90bdadfe-b48b-4b56-bd82-b849f1506868"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 287113/287113 [6:16:35<00:00, 12.71it/s]\n"]}]},{"cell_type":"code","source":["# Export dictionary as json file to data folder\n","with open(f'{data_path}/idf_dict.json', 'w') as out:\n","    json.dump(idf_dict, out)"],"metadata":{"id":"MIoVgosRgd6O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### c. OpenIE\n","\n","*(~60 articles per min)*\n","\n","https://colab.research.google.com/github/stanfordnlp/stanza/blob/master/demo/Stanza_CoreNLP_Interface.ipynb#scrollTo=s194RnNg5z95\n","\n","https://pypi.org/project/stanford-openie/1.0.1/\n","\n","* Input: non-tokenised stories (.txt file)\n","* Output: list of list of list of tuples (.txt file, pickled)"],"metadata":{"id":"JCyMuWu3DXFc"}},{"cell_type":"code","source":["# Download the Stanford CoreNLP package with Stanza's installation command\n","! pip install stanza\n","import stanza\n","stanza.install_corenlp()\n","\n","# Set the CORENLP_HOME environment variable to point to the installation location\n","os.environ[\"CORENLP_HOME\"] = '/content/drive/MyDrive/IP/stanford-corenlp-full-2018-10-05'\n","clear_output()"],"metadata":{"id":"sH3iKesnDzwO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import client module\n","from stanza.server import CoreNLPClient"],"metadata":{"id":"-yUGPRZFTPls"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Construct a CoreNLPClient with the OpenIE annotator and port number 9001\n","client = CoreNLPClient(timeout=150000000, be_quiet=True, annotators=['openie'], \n","endpoint='http://localhost:9001')\n","# Start the background server and wait for some time\n","client.start()\n","time.sleep(10)\n","clear_output()"],"metadata":{"id":"aFp2cotNTcRF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Find relational tuples\n","with open(f'{data_path}/snt_stories.txt', 'r', encoding='latin-1') as file:\n","    stories = file.readlines()\n","    document_triples = []\n","\n","    for idx, story in enumerate(stories):\n","        document = client.annotate(story, output_format='json')\n","        story_triples = []\n","\n","        for sentence in document['sentences']:\n","            sentence_triples = []\n","\n","            for triple in sentence['openie']:\n","                if triple not in sentence_triples:\n","                    # Append triples in format {Relation: [[subject start index, object end index]]}\n","                    sentence_triples.append({triple['relation']: [[triple['subjectSpan'][0], triple['objectSpan'][-1]]]})\n","\n","            if sentence_triples != []:\n","                story_triples.append(sentence_triples)\n","        \n","        if story_triples != []:\n","            document_triples.append(story_triples)"],"metadata":{"id":"LzBvfd8J9S8c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Export list as pickled text file to data folder\n","with open(f'{data_path}/triples.txt', 'wb') as out:\n","    pickle.dump(document_triples, out)"],"metadata":{"id":"UXaMLYlmw96q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### d. (i): MDF-2 keywords"],"metadata":{"id":"nix5cGPYAPyy"}},{"cell_type":"code","source":["# Load MFD-2 dictionary\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer()\n","\n","# Write dictionary\n","with open('/content/drive/MyDrive/mfd2.0.txt','r', encoding='latin-1') as f:\n","    mfd_dict = {}\n","    for line in f:\n","        items = line.split('\\t')\n","        key_1, key_2, value = lemmatizer.lemmatize(items[0]), stemmer.stem(items[0]), items[1].split('\\n')[0]\n","        if key_2 not in mfd_dict.keys() and len(key_2) > 2:\n","            mfd_dict[key_1] = value # lemmas\n","            mfd_dict[key_2] = value # stems"],"metadata":{"id":"mHk8sKOCJ36Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Export dictionary as json file to data folder\n","with open(f'{data_path}/mft_dict.json', 'w') as out:\n","    json.dump(mfd_dict, out)"],"metadata":{"id":"Lts7CjdGrEsw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### d. (ii): BERT similarity embeddings for sentences"],"metadata":{"id":"VgcUgcw59Wze"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertModel\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","clear_output()"],"metadata":{"id":"NuOATn_L9d_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load MFD2.0 dictionary\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer()\n","\n","with open('/content/drive/MyDrive/mfd2.0.txt','r', encoding='latin-1') as f:\n","    mfd_dict = {}\n","    for line in f:\n","        items = line.split('\\t')\n","        key, value = items[0], int(items[1].split('\\n')[0])\n","        mfd_dict[key] = value"],"metadata":{"id":"jSWzNVSDBuPj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make dictionary of BERT word embeddings\n","mfd_embeddings = {}\n","for key in mfd_dict.keys():\n","    word = key\n","    input_ids = tokenizer(key, return_tensors=\"pt\")\n","    output = model(**input_ids)\n","    final_layer = output.last_hidden_state\n","    av_embedding = torch.mean(final_layer, dim=1)\n","    mfd_embeddings[key] = av_embedding"],"metadata":{"id":"WrEMX4tSBbXe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate average foundation embeddings and make dictionary\n","unique_idx = sorted(list(set(mfd_dict.values())))\n","foundation_embeddings = {}\n","for index in unique_idx:\n","    tensor_list = []\n","    for key, value in mfd_dict.items():\n","        if value == index:\n","            tensor_list.append(mfd_embeddings[key])\n","    av_tensor = torch.mean(torch.stack(tensor_list), dim=0)\n","    foundation_embeddings[str(index)] = av_tensor"],"metadata":{"id":"QndzjfYkBm_8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open ('/content/drive/MyDrive/IP/datasets/foundation_embeddings', 'rb') as file:\n","    foundation_embeddings = pickle.load(file)"],"metadata":{"id":"ZLZ6a4YYPDHe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate average sentence embeddings and make dictionary\n","with open('/content/drive/MyDrive/IP/datasets/test_inputs/snt_stories.txt', 'r', encoding='latin-1') as f:\n","    stories = f.readlines()\n","    test_stories = []\n","    for story in stories:\n","        test_stories.append(nltk.tokenize.sent_tokenize(story))"],"metadata":{"id":"LH_V6Lt5IrmP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_embeddings = {}\n","for doc_idx, story in enumerate(test_stories):\n","    story_embeddings = {}\n","    for story_idx, sent in enumerate(story):\n","        input_ids = tokenizer(sent, return_tensors=\"pt\")\n","        output = model(**input_ids)\n","        final_layer = output.last_hidden_state\n","        av_embedding = torch.mean(final_layer, dim=1)\n","        story_embeddings[story_idx] = av_embedding\n","    corpus_embeddings[doc_idx] = story_embeddings\n"],"metadata":{"id":"Eq9pgIGsJgmc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open (f'{data_path}/corpus_embeddings', 'rb') as file:\n","    corpus_embeddings = pickle.load(file)"],"metadata":{"id":"ZMmCDNFhRjg1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Find cosine similarity between each sentence embedding and each foundation embedding\n","\n","mft_embed_dict = corpus_embeddings.copy()\n","cos = torch.nn.CosineSimilarity(eps=1e-6)\n","\n","# Loop over documents and sentences\n","for doc_idx in corpus_embeddings.keys():\n","    for sent_idx in corpus_embeddings[doc_idx].keys():\n","        sent_embed = corpus_embeddings[doc_idx][sent_idx]\n","\n","        cosine_scores = foundation_embeddings.copy()\n","        # Loop over foundations\n","        for key in foundation_embeddings.keys():\n","            cosine_score = cos(sent_embed, foundation_embeddings[key])\n","            cosine_scores[key] = cosine_score.item()\n","    \n","        mft_embed_dict[doc_idx][sent_idx] = cosine_scores\n"],"metadata":{"id":"yMVKdoawT0Qw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open (f'{data_path}/mft_embeddings.json', 'r') as file:\n","    mft_embed_dict = json.load(file)"],"metadata":{"id":"dno-XbdfW17w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\tdef softmax(z):\n","    t = np.exp(z)\n","    a = np.exp(z) / np.sum(t, axis=0)\n","    return a"],"metadata":{"id":"xUBjF7c06Z6B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score_list = []\n","for story_key, story_dict in mft_embed_dict.items():\n","    for sent_key, sent_dict in story_dict.items():\n","        cosine_scores = list(sent_dict.values())\n","        score_list.append(cosine_scores)\n","\n","score_list = np.array(score_list)\n","print(np.mean(score_list))\n","print(np.std(score_list))\n","print(round(np.mean(score_list) + np.std(score_list), 4))\n","        "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtRS8QSj-2Es","executionInfo":{"status":"ok","timestamp":1661106942098,"user_tz":-60,"elapsed":14,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"caebf882-cd8b-4322-e745-dddaf531b5c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.442458133870336\n","0.0904558085993614\n","0.5329\n"]}]},{"cell_type":"markdown","source":["# Stage 2: SGE "],"metadata":{"id":"vkszDTrDrVOB"}},{"cell_type":"markdown","source":["https://github.com/vgupta123/Unsupervised-SAS\n","* Input: AMRs in Proxy text format (.txt file)\n","* Output: predicted summary AMRs and predicted nodes (.txt files)"],"metadata":{"id":"em5OWVklKbgA"}},{"cell_type":"markdown","source":["*Download pipeline input files to Unsupervised SAS folder of the same name to run on VSCode or run with version mounted on Google Drive.*"],"metadata":{"id":"p9galzcZyPp3"}},{"cell_type":"markdown","source":["### Run the algorithm"],"metadata":{"id":"GmAp2SpUBSjQ"}},{"cell_type":"code","source":["# Choose dataset\n","# dataset = 'test'\n","dataset = 'moral_test'\n","data_path = f'/content/drive/MyDrive/IP/datasets/{dataset}_inputs'\n","output_path = f'/content/drive/MyDrive/IP/datasets/{dataset}_outputs'"],"metadata":{"id":"YknZHxxb4E7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ensure correct graph library\n","! pip uninstall -y networkx\n","! pip install networkx==2.3\n","clear_output()"],"metadata":{"id":"U5XvnnR6-USC","executionInfo":{"status":"ok","timestamp":1661707970245,"user_tz":-60,"elapsed":7536,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/IP/u-sas')\n","! python pipeline.py --dataset='{dataset}'"],"metadata":{"id":"-XiCKII65seM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661708387153,"user_tz":-60,"elapsed":60505,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"56c4795c-86d6-4ffd-8e8f-35cd49c6a0da"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","Article 1 done\n","Article 2 done\n","Article 3 done\n","Article 4 done\n","Article 5 done\n","Article 6 done\n","Article 7 done\n","Article 8 done\n","Article 9 done\n","Article 10 done\n","Article 11 done\n","Article 12 done\n","Article 13 done\n","Article 14 done\n","Article 15 done\n","Article 16 done\n","Article 17 done\n","Article 18 done\n","Article 19 done\n","Article 20 done\n","Article 21 done\n","Article 22 done\n","Article 23 done\n","Article 24 done\n","Article 25 done\n","Article 26 done\n","Article 27 done\n","Article 28 done\n","Article 29 done\n","Article 30 done\n","Article 31 done\n","Article 32 done\n","Article 33 done\n","Article 34 done\n","Article 35 done\n","Article 36 done\n","Article 37 done\n","Article 38 done\n","Article 39 done\n","Article 40 done\n","Article 41 done\n","Article 42 done\n","Article 43 done\n","Article 44 done\n","Article 45 done\n","Article 46 done\n","Article 47 done\n","Article 48 done\n","Article 49 done\n","average_ratio 10.573318225278742 mean_deviation 2.0461210771389844\n"]}]},{"cell_type":"code","source":["! cp '/content/drive/MyDrive/IP/u-sas/{dataset}_outputs/predicted_summary_amrs.txt' '{output_path}'\n","! cp '/content/drive/MyDrive/IP/u-sas/{dataset}_outputs/predicted_summary_nodes.txt' '{output_path}'"],"metadata":{"id":"EXapHiTf_kTT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Stage 3: AMR-to-Text generation"],"metadata":{"id":"IyPt9JZ-JwD4"}},{"cell_type":"markdown","source":["https://github.com/bjascob/amrlib\n","\n","* Input: predicted summary AMRs (.txt file)\n","* Output: predicted summary sentences (.txt file)"],"metadata":{"id":"uzVJPJqbegXF"}},{"cell_type":"markdown","source":["### Generate summaries"],"metadata":{"id":"A_6knfTMDQe6"}},{"cell_type":"code","source":["# Instantiate generator: 'generate_t5wtense' model\n","generator = amrlib.load_gtos_model(model_dir='/content/drive/MyDrive/IP/amrlib/data/model_gtos')\n","clear_output()"],"metadata":{"id":"cIMSUajMJ9MT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Full SGE algorithm\n","suffix = ''"],"metadata":{"id":"Z2ZdQCk8hy47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ast import literal_eval\n","pred_summaries = []\n","with open(f'{output_path}{suffix}/predicted_summary_amrs.txt', 'r', encoding='latin-1') as file:\n","    amrs = file.readlines()\n","    for amr in amrs:\n","        # Join them into the correct format for generation\n","        formatted_amr = [\"\\n\".join(literal_eval(amr))]\n","        # Generate text\n","        regenerated, _ = generator.generate(formatted_amr)\n","        string = \"\"\n","        for x in regenerated:\n","            string = string + \" \" + x\n","            text_from_amr = string\n","        pred_summaries.append(text_from_amr)\n","        \n","clear_output()\n","with open(f'{output_path}{suffix}/predicted_summary_sentences.txt', 'w') as out:\n","    for summary in pred_summaries:\n","        out.write(summary + '\\n')\n","\n","# for text in pred_summaries:\n","#     print(text)"],"metadata":{"id":"QqNp8NdVADBA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Post-processing"],"metadata":{"id":"F8zF1zXrKOuM"}},{"cell_type":"code","source":["# # OPTIONAL: load list of predicted summaries\n","# with open(f'{output_path}/predicted_summary_sentences.txt', 'r', encoding='latin-1') as file:\n","#     pred_summaries = file.readlines()"],"metadata":{"id":"rrv45L4W2CY0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load list of reference summaries from output folder\n","with open(f'{output_path}/target_summaries.txt', 'r', encoding='latin-1') as file:\n","    ref_summaries = file.readlines()"],"metadata":{"id":"NCcsTceGdXrQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["to_remove = []\n","duplicate_starts = set()\n","\n","for idx, text in enumerate(pred_summaries):\n","\n","    # Remove if fewer than 10 tokens in summary (probably faulty generation)\n","    tokens = nltk.tokenize.word_tokenize(text)\n","    if len(tokens) < 10:\n","        to_remove.append(idx)\n","    \n","    # Remove duplicates\n","    elif text[:50] in duplicate_starts:\n","        to_remove.append(idx)\n","\n","    # Remove if a word appears more than 10 times\n","    else:\n","        for word in tokens:\n","            if tokens.count(word) > 10:\n","                to_remove.append(idx)\n","                break\n","\n","    duplicate_starts.add(text[:50])\n","\n","# Remove from summary lists\n","if len(to_remove) >= 1:\n","    for idx in sorted(to_remove)[::-1]:\n","        del pred_summaries[idx]\n","        del ref_summaries[idx]"],"metadata":{"id":"S_7Ttc5teQLA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate lengths of summaries\n","pred_len = 0\n","ref_len = 0\n","num_summaries = len(ref_summaries)\n","for i in range(num_summaries):\n","    pred_len += len(nltk.tokenize.word_tokenize(pred_summaries[i]))\n","    ref_len += len(nltk.tokenize.word_tokenize(ref_summaries[i]))\n","\n","print('Number of summaries -> %5.0f' % (num_summaries))\n","print('Total pred words -> %5.0f' % (pred_len))\n","print('Average pred summary words -> %5.1f' % (pred_len / num_summaries))\n","print('Total ref words -> %5.0f' % (ref_len))\n","print('Average ref summary words -> %5.1f' % (ref_len / num_summaries))\n","print('Average pred summary words as percentage of average ref summary words -> %5.1f' %  (100 * pred_len / ref_len))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S45W1EKEgSKu","executionInfo":{"status":"ok","timestamp":1661533095005,"user_tz":-60,"elapsed":5,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"e68e00de-d584-46fe-9ce6-fe8daf836278"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of summaries ->    45\n","Total pred words ->  2452\n","Average pred summary words ->  54.5\n","Total ref words ->  3698\n","Average ref summary words ->  82.2\n","Average pred summary words as percentage of average ref summary words ->  66.3\n"]}]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"8kAxWkIrydop"}},{"cell_type":"markdown","source":["### ROUGE"],"metadata":{"id":"iprY4tvfzoH9"}},{"cell_type":"code","source":["# Calculate average ROUGE scores (P, R, F1) over a dataset\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","average_scores = dict(zip(['rouge1', 'rouge2', 'rougeL'],[[0,0,0],[0,0,0],[0,0,0]]))\n","N = len(ref_summaries)\n","K = len(average_scores.keys())\n","\n","for i in range(N):\n","    scores = scorer.score(ref_summaries[i],\n","                          pred_summaries[i])\n","    for key in scores.keys():\n","        for j in range(K):\n","            average_scores[key][j] += scores[key][j]\n","\n","for key in scores.keys():\n","    for j in range(K):\n","        average_scores[key][j] = round((average_scores[key][j] / N) , 3)\n","\n","# print(average_scores)"],"metadata":{"id":"9-OegzE1zpye"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Node F1 scores"],"metadata":{"id":"GAPEHbVi9gux"}},{"cell_type":"code","source":["# Define function for finding overlapping nodes (non-unique)\n","def intersection(lst1, lst2):\n","    lst3 = list(set(lst1) & set(lst2))\n","    add_nodes = []\n","    for value in lst3:\n","        lst1_count = lst1.count(value)\n","        lst2_count = lst2.count(value)\n","        if lst1_count > 1 and lst2_count > 1:\n","            add_nodes.extend([value] * (min(lst1_count, lst2_count)-1))\n","    lst3.extend(add_nodes)\n","    return lst3"],"metadata":{"id":"NrlwA8hMJz4O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(f'{output_path}{suffix}/predicted_summary_nodes.txt', 'r') as pred_file:\n","    pred_nodes = pred_file.readlines()\n","\n","with open(f'{output_path}{suffix}/target_summary_nodes.txt', 'r') as ref_file:\n","    ref_nodes = ref_file.readlines()\n","\n","if len(to_remove) >= 1 and len(pred_nodes) > len(pred_summaries):\n","    for idx in sorted(to_remove)[::-1]:\n","        del pred_nodes[idx]\n","        del ref_nodes[idx]\n","\n","node_scores = [0,0,0]\n","\n","for i in range(len(pred_nodes)):\n","\n","    ref_tokens = ref_nodes[i].split()\n","    pred_tokens = pred_nodes[i].split()\n","\n","    correct_nodes = intersection(ref_tokens, pred_tokens)\n","    p = len(correct_nodes) / len(pred_tokens)\n","    r = len(correct_nodes) / len(ref_tokens)\n","    if p+r != 0:\n","        f1 = (2*p*r) / (p+r) \n","    node_scores[0] += p\n","    node_scores[1] += r\n","    node_scores[2] += f1\n","\n","for i, score in enumerate(node_scores):\n","    node_scores[i] = round(score / len(pred_nodes), 3)\n","\n","# print(node_scores)"],"metadata":{"id":"fiS3Paee9kqL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### BLEU, CHRF++, Meteor"],"metadata":{"id":"2twm7LYTpezR"}},{"cell_type":"code","source":["chrf = evaluate.load('chrf')\n","meteor = evaluate.load('meteor')\n","bleu = evaluate.load('bleu')\n","clear_output()\n","\n","# Compute BLEU scores from reference and predicted sentences\n","results = bleu.compute(predictions=pred_summaries, references=ref_summaries, smooth=True)\n","bleu = round(results['bleu'],3)\n","# print('BLEU score -> %5.3f' % (results['bleu']))\n","\n","# Compute CHRF++ scores from reference and predicted sentences\n","results = chrf.compute(predictions=pred_summaries, references=ref_summaries, word_order=2)\n","chrf = round(results['score']/100,3)\n","# print('CHRF++ score -> %5.3f' % (results['score']/100))\n","\n","# Compute METEOR scores from reference and predicted sentences\n","results = meteor.compute(predictions=pred_summaries, references=ref_summaries)\n","meteor = round(results['meteor'],3)\n","# print('METEOR score -> %5.3f' % (results['meteor']))"],"metadata":{"id":"AScepG_6peYj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Experiment admin"],"metadata":{"id":"MJhE-oCHF1IF"}},{"cell_type":"code","source":["# Print all results\n","print(average_scores['rouge1'][1],\n","      average_scores['rouge1'][0],\n","      average_scores['rouge2'][1],\n","      average_scores['rouge2'][0],\n","      average_scores['rougeL'][2],\n","      node_scores[2],\n","      bleu,\n","      chrf,\n","      meteor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vk_20CHmDT2Q","executionInfo":{"status":"ok","timestamp":1661108225232,"user_tz":-60,"elapsed":27,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"a6f377c8-2509-43db-9144-0d7bd630a77d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.135 0.35 0.019 0.055 0.121 0.156 0.007 0.133 0.099\n"]}]},{"cell_type":"code","source":["# Copy predictions to archive for safekeeping\n","# Choose number experiment\n","exp = '38'\n","sent_location = f'{output_path}/predicted_summary_sentences.txt'\n","nodes_location = f'{output_path}/predicted_summary_nodes.txt'\n","amrs_location = f'{output_path}/predicted_summary_amrs.txt'\n","archive_location = '/content/drive/MyDrive/IP/datasets/DS predictions'\n","\n","! cp  '{sent_location}'  '{archive_location}'\n","! cp  '{nodes_location}'  '{archive_location}'\n","! cp  '{amrs_location}'  '{archive_location}'\n","! mv '{archive_location}/predicted_summary_sentences.txt' '{archive_location}/{exp}_predicted_summary_sentences.txt'\n","! mv '{archive_location}/predicted_summary_amrs.txt' '{archive_location}/{exp}_predicted_summary_amrs.txt'\n","! mv '{archive_location}/predicted_summary_nodes.txt' '{archive_location}/{exp}_predicted_summary_nodes.txt'"],"metadata":{"id":"UZFeUeM-ww1J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# *Archive*"],"metadata":{"id":"DFD7xbeKQbkI"}},{"cell_type":"markdown","source":["*The archive contiains earlier versions of functions for reference or code that is no longer needed.*"],"metadata":{"id":"LRph2LACfyu5"}},{"cell_type":"markdown","source":["### *Reproduce parser results (SMATCH)*"],"metadata":{"id":"Spyb3KaeMhfH"}},{"cell_type":"code","source":["# Parse some test sentences and write them to a file for testing\n","test_file = '/content/amr_annotation_3.0/data/amrs/split/test/amr-release-3.0-amrs-test-proxy.txt'\n","test_sents = []\n","\n","with open(test_file, encoding = 'utf-8') as infile:\n","    for idx, line in enumerate(infile):\n","        line = line.rstrip()\n","        if '# ::snt' in line:\n","            test_sents.append(line[8:])\n","    \n","    start_time = time.time()\n","    test_penmans = parser.parse_sents(test_sents)\n","    clear_output()\n","    # Should process ~2 sentences/s\n","    print(f'--- Parsed {len(test_sents)} sentences in %s seconds ---' % (time.time() - start_time))\n","\n","with open(f\"/content/drive/MyDrive/IP/datasets/predicted_proxy_penmans.txt\", \"w\") as output:\n","    for penman in test_penmans:\n","        output.write(str(penman) + \"\\n\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o_B4W-pKRvHg","executionInfo":{"status":"ok","timestamp":1659093033286,"user_tz":-60,"elapsed":474446,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"b6e1a6b7-ba1b-4d8a-cb14-eca758992af2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Parsed 823 sentences in 471.56675267219543 seconds ---\n"]}]},{"cell_type":"code","source":["# Compute the SMATCH scores and other metrics\n","from amrlib.evaluate.smatch_enhanced import compute_scores, get_entries\n","test_file = '/content/amr_annotation_3.0/data/amrs/split/test/amr-release-3.0-amrs-test-proxy.txt'\n","predict_file = '/content/drive/MyDrive/IP/datasets/predicted_proxy_penmans.txt'\n","compute_scores(test_file, predict_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oCYJ5ge-MrIz","executionInfo":{"status":"ok","timestamp":1659030604420,"user_tz":-60,"elapsed":55191,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"0ef49be9-2205-439c-e695-168078f3b40d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Smatch           -> P: 0.825,  R: 0.885,  F: 0.854\n","Unlabeled        -> P: 0.848,  R: 0.910,  F: 0.878\n","No WSD           -> P: 0.828,  R: 0.889,  F: 0.857\n","Non_sense_frames -> P: 0.943,  R: 0.936,  F: 0.939\n","Wikification     -> P: 0.000,  R: 0.000,  F: 0.000\n","Named Ent.       -> P: 0.917,  R: 0.929,  F: 0.923\n","Negations        -> P: 0.731,  R: 0.721,  F: 0.726\n","IgnoreVars       -> P: 0.722,  R: 0.803,  F: 0.760\n","Concepts         -> P: 0.925,  R: 0.932,  F: 0.928\n","Frames           -> P: 0.919,  R: 0.913,  F: 0.916\n","Reentrancies     -> P: 0.756,  R: 0.757,  F: 0.757\n","SRL              -> P: 0.834,  R: 0.833,  F: 0.833\n"]}]},{"cell_type":"markdown","source":["### *Reproduce generator results (BLEU, CHRF++, Meteor)*"],"metadata":{"id":"PlxXWl3HE_tW"}},{"cell_type":"code","source":["# Generate sentences from the reference AMRs\n","test_file = '/content/amr_annotation_3.0/data/amrs/split/test/amr-release-3.0-amrs-test-proxy.txt'\n","test_amrs = []\n","amr = ''\n","\n","with open(test_file, encoding = 'utf-8') as infile:\n","    for idx, line in enumerate(infile):\n","        line = line.rstrip()\n","        if '# ::id' in line:\n","            test_amrs.append(amr)\n","            amr = ''\n","        amr = amr + line + '\\n'\n","    test_amrs.append(amr)\n","\n","start_time = time.time()\n","pred_sents, _ = generator.generate(test_amrs[1:], use_tense=False) \n","clear_output()\n","print(f'--- Generated {len(test_amrs[1:])} sentences in %s seconds ---' % (time.time() - start_time))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7nn2T8vFlw0","executionInfo":{"status":"ok","timestamp":1659289745132,"user_tz":-60,"elapsed":66904,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"e4f57c8b-8297-4d96-b9ea-452b10fefe62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Generated 823 sentences in 66.9390721321106 seconds ---\n"]}]},{"cell_type":"code","source":["# Compute BLEU scores from reference and predicted sentences tokenized into words\n","chrf = evaluate.load('chrf')\n","meteor = evaluate.load('meteor')\n","from amrlib.evaluate.bleu_scorer import BLEUScorer\n","\n","test_sents = []\n","with open(test_file, encoding = 'utf-8') as infile:\n","    for idx, line in enumerate(infile):\n","        line = line.rstrip()\n","        if '# ::snt' in line:\n","            test_sents.append(line[8:])\n","\n","test_token_list = []\n","pred_token_list = []\n","for i in range(len(pred_sents)):\n","    test_words = nltk.tokenize.word_tokenize(test_sents[i])\n","    pred_words = nltk.tokenize.word_tokenize(pred_sents[i])\n","    test_token_list.append(test_words)\n","    pred_token_list.append(pred_words)\n","\n","bleu_scorer = BLEUScorer()\n","bleu_score, ref_len, hyp_len = bleu_scorer.compute_bleu(test_token_list, pred_token_list)\n","print('BLEU score -> %5.3f' % (bleu_score))\n","\n","# Compute CHRF++ scores from reference and predicted sentences\n","results = chrf.compute(predictions=pred_sents, references=test_sents, word_order=2)\n","print('CHRF++ score -> %5.3f' % (results['score']/100))\n","\n","# Compute METEOR scores from reference and predicted sentences\n","results = meteor.compute(predictions=pred_sents, references=test_sents)\n","print('METEOR score -> %5.3f' % (results['meteor']))"],"metadata":{"id":"GI13k-6kP788"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### *Transfomer summarisation*"],"metadata":{"id":"CwN8GVIqDIsd"}},{"cell_type":"code","source":["word_counts = []\n","for i in range(len(pred_summaries)):\n","    ref_summ_words = nltk.tokenize.word_tokenize(ref_summaries[i])\n","    word_counts.append(len(ref_summ_words))\n","max(word_counts)"],"metadata":{"id":"4Y2TfldQ7_vu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n","clear_output()"],"metadata":{"id":"Ej4qa6rODPwO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_dataset(filepath):\n","    corpus = pd.read_csv(filepath)\n","    corpus.name = filepath.split('/')[-1][:-4] # Give name\n","    return corpus"],"metadata":{"id":"-980Hqxb3iZp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load from datasets folder in drive\n","with open ('/content/drive/MyDrive/IP/datasets/cnndm_inputs/snt_stories.txt', 'r', encoding='latin-1') as file:\n","    stories = file.readlines()"],"metadata":{"id":"xpFxMj763iZw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformer_summaries = []\n","for story in stories:\n","    try:\n","        summary = summarizer(story, max_length=64, do_sample=False)[0]['summary_text']\n","        print(summary)\n","        transformer_summaries.append(summary)\n","    except IndexError:\n","        print('INDEX ERROR')\n","        transformer_summaries.append(\"None\")\n","        continue"],"metadata":{"id":"t_k7UCo2DUdk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_summaries = transformer_summaries"],"metadata":{"id":"ND35ldJSFxL7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### *Test: SPRING API*"],"metadata":{"id":"UieE9dyo-lof"}},{"cell_type":"code","source":["def sentence_to_amr(sentence_list):\n","    amrs = []\n","    url_base = \"https://nlp.uniroma1.it/spring/api/text-to-amr?sentence=\"\n","    total = len(sentence_list)\n","    with tqdm.tqdm(total=total) as bar:\n","        for sent in sentence_list:\n","            # URL encode the sentence\n","            enc_sentence = urllib.parse.quote(sent)\n","            # Attach it to the get request for the SPRING API\n","            url = url_base + enc_sentence\n","            response = requests.get(url)\n","            amrs.append(response.json())\n","            bar.update(1)\n","    return amrs"],"metadata":{"id":"8XtRmzzWiIhL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["amrs = sentence_to_amr(sent_list)"],"metadata":{"id":"WKYUHeUQBmot"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def amr_to_text(amr_list):\n","    sentences = []\n","    url_base = \"https://nlp.uniroma1.it/spring/api/amr-to-text?penman=\"\n","    total = len(amr_list)\n","    with tqdm.tqdm(total=total) as bar:\n","        for amr in amr_list:\n","            # URL encode the AMR\n","            enc_amr = urllib.parse.quote(amr['penman'])\n","            # Attach it to the get request for the SPRING API\n","            url = url_base + enc_amr\n","            response = requests.get(url)\n","            try:\n","                sentence = response.json()\n","                sentences.append(sentence)\n","            except Exception:\n","                print(\"Decoder error\")\n","            bar.update(1)\n","    return sentences"],"metadata":{"id":"HLjkY3Su8Qxy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["regenerated = amr_to_text(amrs)"],"metadata":{"id":"D8YYWOXv8wyu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["string = \"\"\n","for x in regenerated:\n","    string = string + x['sentence']\n","    article_from_amr = string"],"metadata":{"id":"phf0yCSuDOB_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(article)\n","print(article_from_amr)"],"metadata":{"id":"C_G8n_1s2iPp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### *Combining AMRs and alignments*"],"metadata":{"id":"SIyv9CaB282I"}},{"cell_type":"code","source":["amrs = '/content/amr_annotation_3.0/data/amrs/split/dev/amr-release-3.0-amrs-dev-proxy.txt'\n","alignments = '/content/amr_annotation_3.0/data/alignments/split/dev/amr-release-3.0-alignments-dev-proxy.txt'\n","outfile = '/content/sample_data/combined_dev.txt'\n","\n","with open(alignments, 'r') as alignments:\n","    data = []\n","    data = alignments.readlines()\n","\n","with open(amrs, 'r') as in_1:\n","        with open(outfile, 'w') as out:\n","\n","            for line in in_1:\n","\n","                out.write(line)\n","\n","                if '::id' in line:\n","                    id = line.split('id')[1].split(' ')[1].strip()\n","\n","                    for idx, line in enumerate(data):\n","                        if '::id' in line:\n","                            if line.split('id')[1].split(' ')[1].strip() == id:\n","                                out.write(data[idx+1])\n","                                out.write(data[idx+2])\n"],"metadata":{"id":"cjgIIpZWs_3w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### *Writing predicted AMRs and alignments to input file*"],"metadata":{"id":"VNjl-YfQKZDv"}},{"cell_type":"code","source":["# Parse AMR test sentences\n","test_file = '/content/amr_annotation_3.0/data/amrs/split/test/amr-release-3.0-amrs-test-proxy.txt'\n","test_sents = []\n","\n","with open(test_file, encoding = 'utf-8') as infile:\n","    for idx, line in enumerate(infile):\n","        line = line.rstrip()\n","        if '# ::snt' in line:\n","            test_sents.append(line[8:])\n","    \n","    start_time = time.time()\n","    test_penmans = parser.parse_sents(test_sents)\n","    clear_output()\n","    # Should process ~2 sentences/s\n","    print(f'--- Parsed {len(test_sents)} sentences in %s seconds ---' % (time.time() - start_time))\n","\n","# with open(f\"/content/drive/MyDrive/IP/datasets/predicted_proxy_penmans.txt\", \"w\") as output:\n","#     for penman in test_penmans:\n","#         output.write(str(penman) + \"\\n\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659454878801,"user_tz":-60,"elapsed":480788,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"c8cb4452-737f-4fff-a9e7-422c6f75be4c","id":"kPlU-k3AKR8n"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Parsed 823 sentences in 479.1572017669678 seconds ---\n"]}]},{"cell_type":"code","source":["inference = FAA_Aligner()\n","\n","with open ('/content/drive/MyDrive/IP/datasets/amr_proxy_dict.txt', 'rb') as data:\n","    amr_dict = pickle.load(data)\n","\n","    ref_toks = []\n","    ref_penmans = []\n","    ref_alignments = []\n","    ref_alignments_list = []\n","\n","    for doc in amr_dict:\n","        for sent in doc:\n","              ref_alignments.append(\" \".join(sent['alignments']))\n","              ref_alignments_list.append(sent['alignments'])\n","              ref_toks.append(\"\".join(sent['tok']))\n","              ref_penmans.append(\" \".join(sent['amr']))\n","\n","amr_surface_aligns, test_alignments = inference.align_sents(ref_toks, test_penmans[:-1])\n","test_alignments_list = []\n","for alignment in test_alignments:\n","    test_alignments_list.append(alignment)\n"],"metadata":{"id":"3NKO87QLKnrk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["amrs = '/content/amr_annotation_3.0/data/amrs/split/test/amr-release-3.0-amrs-test-proxy.txt'\n","alignments = '/content/amr_annotation_3.0/data/alignments/split/test/amr-release-3.0-alignments-test-proxy.txt'\n","outfile = '/content/sample_data/proxy_pred_amrs.txt'\n","\n","# Write input with predicted penmans but reference alignments\n","with open(alignments, 'r') as alignments:\n","    data = []\n","    data = alignments.readlines()\n","\n","with open(amrs, 'r') as in_1:\n","    with open(outfile, 'w') as out:\n","        \n","        out.write('AMR release; corpus: proxy; section: test; number of AMRs: 823' + '\\n\\n')\n","        counter = 0\n","        for line in in_1:\n","\n","            if line.startswith('# ::') and 'snt ' not in line:\n","                out.write(line)\n","\n","            if '::id' in line:\n","                id = line.split('id')[1].split(' ')[1].strip()\n","\n","                for idx, line in enumerate(data):\n","                    if '::id' in line:\n","                        if line.split('id')[1].split(' ')[1].strip() == id:\n","                            out.write(data[idx+1])\n","                            out.write(data[idx+2])\n","\n","            if line.startswith('('):\n","                for idx, penman_line in enumerate(test_penmans[counter]):\n","                    out.write(penman_line)\n","                out.write('\\n\\n')\n","                counter += 1\n"],"metadata":{"id":"9BDtqUCdLRUJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_penmans[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"EIgZAukCZ6Cn","executionInfo":{"status":"ok","timestamp":1659458014338,"user_tz":-60,"elapsed":17,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"5b24d248-24f6-485d-ef15-72da9bf62eea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'# ::snt 2007-08-21\\n(d / date-entity\\n      :year 2007\\n      :month 8\\n      :day 21)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["amrs = '/content/amr_annotation_3.0/data/amrs/split/test/amr-release-3.0-amrs-test-proxy.txt'\n","alignments = '/content/amr_annotation_3.0/data/alignments/split/test/amr-release-3.0-alignments-test-proxy.txt'\n","outfile = '/content/sample_data/proxy_pred_amrs_2.txt'\n","\n","# Write input with predicted penmans and predicted alignments\n","with open(alignments, 'r') as alignments:\n","    data = []\n","    data = alignments.readlines()\n","\n","with open(amrs, 'r') as in_1:\n","    with open(outfile, 'w') as out:\n","        \n","        out.write('AMR release; corpus: proxy; section: test; number of AMRs: 823' + '\\n\\n')\n","        counter = 0\n","        for line in in_1:\n","\n","            if line.startswith('# ::') and 'snt ' not in line:\n","                out.write(line)\n","\n","            if '::id' in line:\n","                id = line.split('id')[1].split(' ')[1].strip()\n","\n","                for idx, line in enumerate(data):\n","                    if '::id' in line:\n","                        if line.split('id')[1].split(' ')[1].strip() == id:\n","                            out.write(data[idx+1])\n","                            out.write(f'# ::alignments {test_alignments_list[counter]}'+'\\n')\n","\n","            if line.startswith('('):\n","                for penman_line in test_penmans[counter]:\n","                    out.write(penman_line)\n","                out.write('\\n\\n')\n","                counter += 1\n","                \n","            if counter == (len(test_alignments_list) - 1):\n","                break\n","\n","\n","        "],"metadata":{"id":"Xgt5e6OJSM43"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### *Export selection of articles from corpus*"],"metadata":{"id":"fdetCPVdl3Ri"}},{"cell_type":"code","source":["# Cell for choosing and exporting random articles from corpus\n","\n","import random\n","\n","df_temp = pd.DataFrame(corpus)\n","\n","random_ints = []\n","while len(random_ints) < 50:\n","    i = random.randint(0,len(df_temp))\n","    if i not in random_ints:\n","        random_ints.append(i)\n","print(random_ints)\n","random_select = df_temp.iloc[random_ints]\n","random_select.to_csv('/content/drive/MyDrive/IP/datasets/test_selection.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KJvt1CEVZYfQ","executionInfo":{"status":"ok","timestamp":1659379788526,"user_tz":-60,"elapsed":1377,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"b0ef7aae-6e80-4a85-eafd-b7ee2db96b2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2866, 2443, 2382, 1766, 1474, 4807, 2100, 1048, 3154, 1448, 539, 1764, 2996, 1728, 4113, 875, 2076, 3428, 4456, 3525, 881, 1717, 418, 1801, 2332, 1373, 1478, 4478, 812, 2901, 4068, 745, 1128, 1684, 799, 1970, 3702, 3917, 1020, 1083, 3501, 3970, 2861, 3119, 527, 582, 2785, 2794, 4110, 165]\n"]}]},{"cell_type":"markdown","source":["### *Processing to dataframes*"],"metadata":{"id":"KnhuiN1Fi3-b"}},{"cell_type":"code","source":["# Function for parsing sentences of an article and compiling a dataframe\n","def process_corpus(corpus, start_idx, end_idx, key='article', visualise=False, regenerate=False):\n","    \"\"\" Takes a corpus as a Huggingface dataset and returns a subset as a Pandas \n","        dataframe with: \n","            (1) a list of Penman format AMRs for each sentence, \n","            (2) a list of graphical AMRs for each sentence (optional)\n","            (3) a regenerated article (optional).\n","    \"\"\"\n","    start_time = time.time()\n","    num_records = int(end_idx - start_idx)\n","    indices = list(range(start_idx, end_idx))\n","    corpus_subset = corpus.select(indices=indices)\n","\n","    total = num_records\n","    with tqdm.tqdm(total=total) as bar:\n","        \n","        sentences = []\n","        penmans = []\n","        graphs = []\n","        regen = []\n","\n","        for art in corpus_subset:\n","\n","            # Preprocess the article\n","            sentence_list = preprocess(art[key])\n","            sentences.append(sentence_list)\n","\n","            print('done')\n","\n","            # Parse each sentence to an AMR\n","            penman_list = parser.parse_sents(sentence_list)\n","            penmans.append(penman_list)\n","\n","            print('done')\n","            \n","\n","            # Draw a graph for each sentence\n","            if visualise is True: \n","                graph_list = []\n","                try:\n","                    for penman in penman_list:\n","                        graph = produce_graph(penman)\n","                        graph_list.append(graph)\n","                    graphs.append(graph_list)\n","                except IndexError:\n","                    print(\"Could not draw graph.\")\n","                    graphs.append(\"Missing graph\")\n","                \n","\n","            # Regenerate the article from the AMRs\n","            if regenerate is True:\n","                regenerated, _ = generator.generate(penman_list)\n","                string = \"\"\n","                for x in regenerated:\n","                    string = string + \" \" + x\n","                    text_from_amr = string\n","                regen.append(text_from_amr)       \n","\n","            bar.update(1)\n","\n","    # Convert to dataframe in order to add graph column\n","    corpus_subset = pd.DataFrame(corpus_subset)\n","    corpus_subset[\"sentences\"] = sentences\n","    corpus_subset[\"penmans\"] = penmans\n","    if visualise is True:\n","        corpus_subset[\"graphs\"] = graphs\n","    if regen is True:\n","        corpus_subset[\"regen\"] = regen\n","\n","    clear_output()\n","\n","    print(f'---Processed {num_records} {key} in {(time.time() - start_time)}seconds ---')\n","\n","    corpus_subset.name = f'{key}_{start_idx}_{end_idx}'\n","\n","    return corpus_subset"],"metadata":{"id":"h5hdTatGXxar"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### *Understanding OpenIE*"],"metadata":{"id":"nx3yYJL452hv"}},{"cell_type":"code","source":["text = \"Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity.\"\n","document = client.annotate(text, output_format='json')\n","triples = []\n","for sentence in document['sentences']:\n","    for triple in sentence['openie']:\n","        triples.append({\n","           'subject': triple['subject'],\n","           'relation': triple['relation'],\n","            'object': triple['object']\n","        })\n","print(triples)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4n8kr1yT1gN","executionInfo":{"status":"ok","timestamp":1658763481456,"user_tz":-60,"elapsed":372,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"902cbcff-08e3-4626-aeaa-70913167eb78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'subject': 'Albert Einstein', 'relation': 'was', 'object': 'theoretical physicist'}, {'subject': 'Einstein', 'relation': 'was', 'object': 'born'}, {'subject': 'Albert Einstein', 'relation': 'was', 'object': 'born theoretical physicist'}, {'subject': 'He', 'relation': 'developed', 'object': 'theory of relativity'}, {'subject': 'He', 'relation': 'developed', 'object': 'theory'}]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"P6Sb4HU4Agv2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### *Redundant TF-IDF*"],"metadata":{"id":"Ff85PNKyPzHZ"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer"],"metadata":{"id":"Cf6GMETnr2FP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tfIdfVectorizer=TfidfVectorizer(use_idf=True, stop_words=stopwords)\n","tfIdf = tfIdfVectorizer.fit_transform(corpus)\n","df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n","idf = df.to_dict()\n","idf = idf['TF-IDF']\n","print(idf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PdKomUhUsdfO","executionInfo":{"status":"ok","timestamp":1658172011945,"user_tz":-60,"elapsed":291,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"a35520b5-0c2d-4023-9b14-3964c4f233cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'000': 0.0, '000000': 0.0, '001100': 0.0, '010000': 0.0, '010100': 0.0, '010200': 0.0, '011005': 0.0, '011006': 0.0, '020000': 0.0, '020200': 0.0, '020700': 0.0, '020718': 0.0, '020724': 0.0, '020725': 0.0, '030000': 0.0, '030500': 0.0, '030600': 0.0, '030622': 0.0, '030624': 0.0, '031107': 0.0, '031108': 0.0, '040000': 0.0, '040403': 0.0, '040922': 0.0, '050000': 0.0, '050400': 0.0, '050500': 0.0, '050600': 0.0, '060000': 0.0, '08': 0.0, '080200': 0.0, '081000': 0.0, '081113': 0.0, '081114': 0.0, '09': 0.0, '10': 0.0, '100': 0.0, '1000': 0.0, '100000': 0.0, '103': 0.0, '105': 0.0, '11': 0.03262824797253317, '11000': 0.0, '12': 0.035617535939241995, '1200': 0.0, '14': 0.0, '150': 0.0, '16': 0.0, '160': 0.0, '17': 0.0, '1730': 0.0, '18': 0.0, '186': 0.0, '19': 0.030309576088993935, '1960': 0.0, '1970': 0.0, '1980': 0.0, '1988': 0.0, '1990': 0.0, '1995': 0.0, '1996': 0.0, '1998': 0.0, '20': 0.0, '200': 0.0, '2000': 0.0, '2001': 0.0, '20010400': 0.0, '20011126': 0.0, '2002': 0.0, '20020828': 0.0, '20020829': 0.0, '20020903': 0.0, '20020904': 0.0, '20020916': 0.0, '20021113': 0.0, '2003': 0.0, '2004': 0.0, '20040718': 0.0, '20040824': 0.0, '20041000': 0.0, '2005': 0.0, '2007': 0.2273206657322334, '2008': 0.0, '20080400': 0.0, '20080500': 0.0, '20080726': 0.0, '2009': 0.0, '2010': 0.0, '21': 0.11949210208573798, '22': 0.0, '23': 0.0, '24': 0.0, '25': 0.0, '26': 0.0, '260': 0.0, '27': 0.0, '28': 0.0, '29': 0.0, '296': 0.0, '30': 0.028415083216529174, '300': 0.0, '3000': 0.0, '300000': 0.0, '33': 0.0, '34': 0.0, '35': 0.035617535939241995, '350': 0.0, '3500': 0.0, '3700': 0.0, '40': 0.0, '400': 0.0, '41': 0.0, '4128': 0.0, '42': 0.0, '4400': 0.0, '45': 0.0, '480': 0.0, '50': 0.0, '5000': 0.0, '50000': 0.0, '500000': 0.0, '570000': 0.0, '600': 0.0, '6000': 0.0, '650': 0.0, '696': 0.0, '70': 0.0, '7000': 0.0, '71': 0.0, '72': 0.0, '75': 0.0, '790000': 0.0, '815': 0.0, '825': 0.0, '83': 0.0, '850': 0.0, '8500': 0.0, '90': 0.0, '920000': 0.0, '950000': 0.0, '970000': 0.0, '980000': 0.0, '980800': 0.0, '99': 0.0, '990400': 0.0, 'abides': 0.0, 'able': 0.0, 'aboard': 0.0, 'aborted': 0.0, 'abs': 0.0, 'absence': 0.0, 'abu': 0.0, 'abuse': 0.0, 'academy': 0.0, 'accepted': 0.0, 'access': 0.06525649594506634, 'accidental': 0.0, 'accord': 0.03983070069524599, 'accordance': 0.0, 'according': 0.025425795249820346, 'account': 0.035617535939241995, 'accountancy': 0.0, 'accounts': 0.0, 'accusatory': 0.0, 'accused': 0.0, 'achievements': 0.0, 'acquaint': 0.0, 'acquire': 0.0, 'acre': 0.0, 'across': 0.0, 'act': 0.0, 'acted': 0.0, 'action': 0.0, 'actions': 0.0, 'active': 0.0, 'actively': 0.0, 'activists': 0.0, 'activities': 0.04242526098763271, 'activity': 0.0, 'acts': 0.0, 'actual': 0.0, 'adamant': 0.0, 'add': 0.0, 'added': 0.0, 'addition': 0.0, 'additional': 0.0, 'address': 0.030309576088993935, 'adherent': 0.0, 'administering': 0.0, 'administration': 0.030309576088993935, 'administrative': 0.0, 'admission': 0.0, 'admitted': 0.0, 'adopt': 0.0, 'adopting': 0.0, 'advance': 0.0, 'advanced': 0.0, 'advised': 0.0, 'adviser': 0.0, 'advocated': 0.0, 'affair': 0.0, 'affairs': 0.0, 'affect': 0.0, 'affected': 0.0, 'afghan': 0.0, 'afghanistan': 0.0, 'africa': 0.0, 'african': 0.0, 'africans': 0.0, 'afterward': 0.0, 'age': 0.0, 'agency': 0.08485052197526542, 'agenda': 0.0, 'agents': 0.0, 'aggression': 0.0, 'aggressive': 0.0, 'aggressively': 0.0, 'aggressor': 0.0, 'agree': 0.0, 'agreed': 0.0, 'agreement': 0.1386427401976867, 'agreements': 0.0, 'agrees': 0.0, 'agricultural': 0.0, 'agriculture': 0.0, 'ahead': 0.0, 'ahmadinejad': 0.0, 'ahmed': 0.0, 'aid': 0.0, 'aiding': 0.0, 'aids': 0.0, 'aimed': 0.0, 'aims': 0.0, 'air': 0.0, 'aircraft': 0.0, 'airspace': 0.0, 'airstrike': 0.0, 'airstrikes': 0.0, 'ak': 0.0, 'al': 0.0, 'alarm': 0.0, 'alberto': 0.0, 'albright': 0.03262824797253317, 'alert': 0.0, 'ali': 0.030309576088993935, 'alignment': 0.0, 'allay': 0.0, 'alleged': 0.0, 'allegedly': 0.0, 'allies': 0.0, 'alliot': 0.0, 'allow': 0.0, 'allowed': 0.0, 'allowing': 0.0, 'allows': 0.0, 'ally': 0.0, 'along': 0.0, 'already': 0.0, 'also': 0.0, 'alternative': 0.0, 'alto': 0.0, 'amaru': 0.0, 'amazon': 0.0, 'ambiguities': 0.03983070069524599, 'ambush': 0.0, 'ambushed': 0.0, 'america': 0.0, 'american': 0.0, 'ammunition': 0.0, 'among': 0.0, 'amorim': 0.0, 'amount': 0.0, 'amounted': 0.0, 'amounts': 0.0, 'analyst': 0.0, 'analysts': 0.0, 'analyzes': 0.0, 'angular': 0.0, 'anhui': 0.0, 'annexation': 0.0, 'announce': 0.0, 'announced': 0.0, 'announcement': 0.0, 'annual': 0.0, 'anonymity': 0.0, 'anonymously': 0.0, 'another': 0.026813312315963782, 'answering': 0.0, 'answers': 0.03262824797253317, 'anthrax': 0.0, 'anti': 0.0, 'antich': 0.0, 'anticipated': 0.0, 'antonio': 0.0, 'anyone': 0.0, 'anything': 0.0, 'anzoategui': 0.0, 'apparently': 0.0, 'appeal': 0.0, 'appeared': 0.0, 'application': 0.0, 'applied': 0.0, 'applies': 0.0, 'appreciated': 0.0, 'approach': 0.0, 'approaching': 0.0, 'appropriate': 0.0, 'approval': 0.0, 'approved': 0.0, 'approximately': 0.0, 'april': 0.0, 'arab': 0.0, 'arabia': 0.0, 'arak': 0.0, 'archives': 0.0, 'area': 0.0, 'areas': 0.0, 'argentina': 0.0, 'arm': 0.0, 'armada': 0.0, 'armaments': 0.0, 'armed': 0.0, 'arming': 0.0, 'armored': 0.0, 'arms': 0.03778791722055424, 'army': 0.0, 'around': 0.0, 'arrears': 0.0, 'arrested': 0.0, 'arrests': 0.0, 'arrived': 0.0, 'arroyo': 0.0, 'arsenal': 0.0, 'articulated': 0.0, 'artillery': 0.0, 'artists': 0.0, 'asghar': 0.03983070069524599, 'asia': 0.0, 'asian': 0.0, 'ask': 0.0, 'asked': 0.0, 'aspect': 0.0, 'assassination': 0.0, 'assassinations': 0.0, 'assault': 0.0, 'assembling': 0.0, 'assembly': 0.0, 'asserted': 0.0, 'asserting': 0.0, 'assertion': 0.0, 'assessment': 0.0, 'assessments': 0.0, 'asset': 0.0, 'assistance': 0.0, 'assistant': 0.0, 'associated': 0.0, 'associates': 0.0, 'association': 0.03262824797253317, 'assodalah': 0.0, 'asymmetrical': 0.0, 'atomic': 0.05085159049964069, 'attach': 0.0, 'attack': 0.0, 'attacked': 0.0, 'attacks': 0.0, 'attempt': 0.0, 'attempted': 0.0, 'attempting': 0.0, 'attended': 0.0, 'attorney': 0.0, 'attributed': 0.0, 'audited': 0.03983070069524599, 'august': 0.16087987389578268, 'australia': 0.0, 'australian': 0.0, 'austria': 0.0, 'authorities': 0.0, 'authority': 0.0, 'authorized': 0.0, 'autonomous': 0.0, 'available': 0.0, 'aviv': 0.0, 'avoid': 0.0, 'aware': 0.0, 'ayatollah': 0.0, 'back': 0.0, 'backing': 0.0, 'bacteria': 0.0, 'bacteriological': 0.0, 'bad': 0.0, 'baghdad': 0.0, 'bahadur': 0.0, 'balance': 0.0, 'balanced': 0.0, 'balancing': 0.0, 'bali': 0.0, 'ballistic': 0.0, 'ban': 0.0, 'banco': 0.0, 'bank': 0.0, 'barack': 0.0, 'barracks': 0.0, 'base': 0.0, 'based': 0.024201918460525183, 'bases': 0.0, 'basic': 0.0, 'basis': 0.0, 'bath': 0.0, 'battery': 0.0, 'bazaar': 0.0, 'beatty': 0.0, 'became': 0.0, 'become': 0.0, 'becoming': 0.0, 'began': 0.03262824797253317, 'begin': 0.0, 'begun': 0.0, 'behalf': 0.0, 'beira': 0.0, 'belgian': 0.0, 'believe': 0.0, 'believed': 0.0, 'belokolos': 0.0, 'belong': 0.0, 'beneficial': 0.0, 'benefit': 0.0, 'benefits': 0.0, 'berlin': 0.03983070069524599, 'better': 0.0, 'beyond': 0.0, 'bid': 0.0, 'big': 0.0, 'bilateral': 0.0, 'bill': 0.0, 'billion': 0.0, 'bills': 0.0, 'bin': 0.0, 'biological': 0.0, 'biomedical': 0.0, 'biotechnology': 0.0, 'biowarfare': 0.0, 'bioweapons': 0.0, 'bishalbazar': 0.0, 'bishkek': 0.0, 'bitter': 0.0, 'black': 0.0, 'blacklisted': 0.0, 'block': 0.0, 'blocked': 0.0, 'board': 0.03262824797253317, 'bolivarian': 0.0, 'bolivia': 0.0, 'bomb': 0.0909287282669818, 'bombed': 0.0, 'bombings': 0.0, 'bombs': 0.0, 'border': 0.0, 'borders': 0.0, 'born': 0.0, 'boss': 0.0, 'bought': 0.0, 'boundary': 0.0, 'branch': 0.0, 'brasilia': 0.0, 'brazil': 0.0, 'brazilian': 0.0, 'brazilians': 0.0, 'breakthrough': 0.0, 'bribery': 0.0, 'bribes': 0.0, 'bribing': 0.0, 'brief': 0.0, 'briefed': 0.0, 'brisbane': 0.0, 'british': 0.0, 'broad': 0.0, 'broadly': 0.0, 'broke': 0.0, 'brother': 0.0, 'brought': 0.0, 'budget': 0.0, 'budgeted': 0.0, 'build': 0.03983070069524599, 'building': 0.0, 'buildings': 0.0, 'buildup': 0.0, 'built': 0.0, 'bulletproof': 0.0, 'bumper': 0.0, 'bureau': 0.0, 'burglary': 0.0, 'burned': 0.0, 'burns': 0.0, 'bush': 0.030309576088993935, 'business': 0.0, 'businesses': 0.0, 'businessman': 0.0, 'businessmen': 0.0, 'busy': 0.0, 'buy': 0.0, 'buying': 0.0, 'cadre': 0.0, 'calculation': 0.0, 'caliber': 0.0, 'callao': 0.0, 'called': 0.0, 'came': 0.0, 'campaign': 0.0, 'campos': 0.0, 'campus': 0.0, 'capabilities': 0.0, 'capability': 0.035617535939241995, 'capacities': 0.0, 'capacity': 0.0, 'cape': 0.0, 'capital': 0.0, 'capitalist': 0.0, 'capitol': 0.0, 'captured': 0.0, 'cardoso': 0.0, 'cargo': 0.0, 'carlos': 0.0, 'carried': 0.0, 'carrier': 0.0, 'carrying': 0.0, 'cas': 0.0, 'case': 0.0, 'cases': 0.0, 'casey': 0.07123507187848399, 'cash': 0.0, 'casks': 0.0, 'caspian': 0.0, 'castro': 0.0, 'casualty': 0.0, 'caucasus': 0.0, 'caucus': 0.0, 'cause': 0.0, 'caused': 0.0, 'causes': 0.0, 'cbn': 0.0, 'cease': 0.0, 'ceding': 0.03983070069524599, 'cells': 0.0, 'celso': 0.0, 'cement': 0.0, 'center': 0.0, 'centers': 0.0, 'central': 0.0, 'centrifuges': 0.0, 'century': 0.0, 'certain': 0.0, 'certainty': 0.0, 'chaired': 0.0, 'chairman': 0.0, 'challenges': 0.0, 'chancellor': 0.0, 'channel': 0.0, 'channels': 0.0, 'charge': 0.0, 'charged': 0.0, 'charges': 0.0, 'charter': 0.0, 'chavez': 0.0, 'chavista': 0.0, 'chechnya': 0.0, 'check': 0.0, 'chemical': 0.0, 'chen': 0.0, 'chief': 0.0, 'child': 0.0, 'children': 0.0, 'chile': 0.0, 'china': 0.0, 'chinese': 0.024201918460525183, 'ching': 0.0, 'chirac': 0.0, 'chirazi': 0.0, 'choose': 0.0, 'chosen': 0.0, 'christopher': 0.0, 'chrysler': 0.0, 'chtf': 0.0, 'chu': 0.0, 'chuen': 0.0, 'chung': 0.0, 'cigarettes': 0.0, 'circles': 0.0, 'circulated': 0.0, 'cited': 0.0, 'citizen': 0.0, 'citizens': 0.0, 'city': 0.0, 'cityu': 0.0, 'civil': 0.0, 'civilian': 0.03262824797253317, 'civilians': 0.0, 'claim': 0.028415083216529174, 'claimed': 0.0, 'claims': 0.0, 'clandestine': 0.0, 'clash': 0.0, 'classic': 0.0, 'classified': 0.0, 'clean': 0.0, 'clear': 0.0, 'clip': 0.0, 'close': 0.0, 'closed': 0.0, 'closely': 0.0, 'closer': 0.0, 'closest': 0.0, 'closing': 0.03983070069524599, 'co': 0.0, 'coal': 0.0, 'coalition': 0.0, 'coast': 0.0, 'cocaine': 0.0, 'coined': 0.0, 'cold': 0.0, 'collaborated': 0.0, 'collaboration': 0.035617535939241995, 'collapse': 0.0, 'collapsed': 0.0, 'collected': 0.0, 'collecting': 0.0, 'collects': 0.0, 'college': 0.0, 'colombia': 0.0, 'colombian': 0.0, 'colonel': 0.0, 'colony': 0.0, 'column': 0.0, 'com': 0.0, 'combat': 0.0, 'combatants': 0.0, 'combating': 0.0, 'come': 0.0, 'comedian': 0.0, 'comercio': 0.0, 'coming': 0.0, 'commandant': 0.0, 'commander': 0.0, 'commanders': 0.0, 'comment': 0.0, 'commentators': 0.0, 'commercial': 0.0, 'commercialization': 0.0, 'commission': 0.0, 'commissioner': 0.0, 'commissions': 0.0, 'commit': 0.0, 'committee': 0.0, 'common': 0.0, 'communication': 0.0, 'communique': 0.0, 'community': 0.030309576088993935, 'companies': 0.0, 'company': 0.0, 'compensation': 0.0, 'complains': 0.0, 'complement': 0.0, 'complete': 0.0, 'complex': 0.0, 'complicate': 0.0, 'complicity': 0.0, 'components': 0.0, 'composed': 0.0, 'compound': 0.0, 'comprehensive': 0.0, 'comprehensively': 0.0, 'compressors': 0.0, 'comprises': 0.0, 'compromise': 0.0, 'computers': 0.0, 'concentrated': 0.035617535939241995, 'concern': 0.0, 'concerned': 0.0, 'concerns': 0.0, 'concluding': 0.0, 'condemned': 0.0, 'condition': 0.0, 'conditions': 0.0, 'condoleezza': 0.0, 'conducive': 0.0, 'conduct': 0.0, 'conducted': 0.0, 'conduit': 0.0, 'conference': 0.0, 'confidence': 0.035617535939241995, 'confidently': 0.0, 'confirmed': 0.0, 'conflict': 0.0, 'conflicted': 0.0, 'conform': 0.0, 'confrontation': 0.0, 'confucian': 0.0, 'congress': 0.0, 'congressman': 0.0, 'connection': 0.0, 'connections': 0.0, 'consensus': 0.0, 'consented': 0.0, 'consider': 0.0, 'considerably': 0.0, 'considered': 0.0, 'considering': 0.0, 'considers': 0.03262824797253317, 'consisted': 0.0, 'consistently': 0.0, 'consists': 0.0, 'constantly': 0.0, 'constituted': 0.0, 'constitutes': 0.0, 'constitution': 0.0, 'constitutionality': 0.0, 'construction': 0.0, 'consult': 0.0, 'consultations': 0.0, 'consulted': 0.0, 'contact': 0.0, 'contained': 0.03983070069524599, 'context': 0.0, 'continent': 0.0, 'continental': 0.0, 'continents': 0.0, 'continue': 0.03262824797253317, 'continued': 0.0, 'continues': 0.035617535939241995, 'continuing': 0.03983070069524599, 'continuously': 0.0, 'contradicts': 0.0, 'contrary': 0.0, 'contribute': 0.0, 'contributed': 0.0, 'contributions': 0.0, 'control': 0.036446685054215056, 'controlled': 0.035617535939241995, 'controls': 0.0, 'controversial': 0.0, 'convention': 0.0, 'conventional': 0.0, 'conversion': 0.0, 'convert': 0.0, 'converted': 0.0, 'convicted': 0.0, 'convinced': 0.0, 'cooled': 0.0, 'cooperate': 0.0, 'cooperated': 0.0, 'cooperating': 0.0, 'cooperation': 0.021212630493816356, 'coordination': 0.0, 'coordinators': 0.0, 'copies': 0.03983070069524599, 'cordoned': 0.0, 'core': 0.0, 'corporate': 0.0, 'corporation': 0.0, 'corporations': 0.0, 'correctness': 0.0, 'correspondents': 0.0, 'corridor': 0.0, 'corrupt': 0.0, 'corruption': 0.0, 'costa': 0.0, 'costly': 0.0, 'could': 0.07289337010843011, 'council': 0.09680767384210073, 'counter': 0.0, 'counteract': 0.0, 'counterattack': 0.0, 'counterpart': 0.0, 'counterweight': 0.0, 'countries': 0.0, 'country': 0.0, 'coup': 0.0, 'couples': 0.0, 'courageous': 0.0, 'cover': 0.0, 'covering': 0.0, 'covert': 0.0, 'crackdown': 0.0, 'crackdowns': 0.0, 'cracked': 0.0, 'craft': 0.0, 'crashed': 0.0, 'crashes': 0.0, 'create': 0.0, 'created': 0.0, 'creating': 0.0, 'crew': 0.0, 'crime': 0.0, 'crimes': 0.0, 'criminal': 0.0, 'criminals': 0.0, 'crisis': 0.0, 'criteria': 0.0, 'criterion': 0.0, 'critical': 0.0, 'criticism': 0.07966140139049198, 'critics': 0.0, 'crop': 0.0, 'crops': 0.0, 'cross': 0.0, 'crucial': 0.0, 'cryogenic': 0.0, 'cuban': 0.0, 'culminated': 0.0, 'cultivation': 0.0, 'cunyi': 0.0, 'curb': 0.0, 'curbs': 0.0, 'current': 0.0, 'currently': 0.0, 'customers': 0.0, 'cut': 0.0, 'cutoff': 0.0, 'cutting': 0.0, 'cyber': 0.0, 'cycle': 0.0, 'da': 0.0, 'daily': 0.0, 'daimler': 0.0, 'damage': 0.0, 'damaged': 0.0, 'dance': 0.0, 'danger': 0.0, 'dangerous': 0.0, 'daryl': 0.035617535939241995, 'data': 0.0, 'datanalisis': 0.0, 'david': 0.030309576088993935, 'dawei': 0.0, 'day': 0.0, 'de': 0.0, 'dead': 0.0, 'deadline': 0.0, 'deadly': 0.0, 'deal': 0.0, 'dealer': 0.0, 'dealing': 0.0, 'deals': 0.0, 'death': 0.0, 'debate': 0.0, 'decades': 0.0, 'december': 0.0, 'decided': 0.0, 'decision': 0.0, 'decisions': 0.0, 'declare': 0.0, 'declared': 0.0, 'declaring': 0.0, 'declined': 0.0, 'decommissioning': 0.0, 'decreasing': 0.0, 'decree': 0.0, 'deepen': 0.0, 'deeply': 0.0, 'defaced': 0.0, 'defend': 0.0, 'defending': 0.0, 'defense': 0.0, 'defensive': 0.0, 'defiance': 0.03983070069524599, 'defied': 0.0, 'deflect': 0.03983070069524599, 'defrosted': 0.0, 'defrosting': 0.0, 'defuse': 0.0, 'degrees': 0.0, 'delay': 0.0, 'delays': 0.0, 'delegation': 0.0, 'deliberate': 0.0, 'deliver': 0.0, 'delta': 0.0, 'demand': 0.035617535939241995, 'demanded': 0.0, 'demanding': 0.0, 'demands': 0.035617535939241995, 'democracy': 0.0, 'democratic': 0.0, 'demonstrated': 0.0, 'demonstrates': 0.0, 'deniability': 0.0, 'denials': 0.0, 'denied': 0.0, 'denying': 0.0, 'department': 0.028415083216529174, 'depend': 0.0, 'depository': 0.0, 'depressing': 0.0, 'deputy': 0.0, 'dereliction': 0.0, 'describe': 0.0, 'described': 0.0, 'descriptors': 0.0, 'design': 0.035617535939241995, 'designed': 0.0, 'despite': 0.0, 'destination': 0.0, 'destroy': 0.0, 'destroyed': 0.0, 'destruction': 0.0, 'detail': 0.0, 'detailed': 0.0, 'details': 0.03262824797253317, 'detained': 0.0, 'detected': 0.0, 'deteriorating': 0.0, 'determination': 0.0, 'detractors': 0.0, 'deuba': 0.0, 'devalue': 0.0, 'devastating': 0.0, 'develop': 0.0, 'developed': 0.0, 'developing': 0.0, 'development': 0.02038091129479884, 'developments': 0.0, 'deviance': 0.0, 'devised': 0.0, 'devoted': 0.0, 'dialogue': 0.0, 'dialogues': 0.0, 'diamonds': 0.0, 'die': 0.0, 'diesel': 0.0, 'dieter': 0.0, 'differed': 0.0, 'different': 0.0, 'difficult': 0.0, 'difficulties': 0.0, 'dimension': 0.0, 'diplomat': 0.0, 'diplomatic': 0.0, 'diplomatically': 0.0, 'diplomats': 0.0, 'direct': 0.0, 'directed': 0.0, 'directly': 0.0, 'director': 0.025425795249820346, 'disable': 0.0, 'disabling': 0.0, 'disagreement': 0.0, 'disarmament': 0.0, 'discipline': 0.0, 'disclosed': 0.0, 'disco': 0.0, 'discounted': 0.0, 'discuss': 0.0, 'discussed': 0.0, 'discussions': 0.0, 'dismantle': 0.0, 'dismissed': 0.0, 'dispatch': 0.0, 'dispatched': 0.0, 'displayed': 0.0, 'disposal': 0.0, 'dispute': 0.0, 'disputes': 0.0, 'disrupt': 0.0, 'dissuade': 0.035617535939241995, 'dissuaded': 0.0, 'distressing': 0.0, 'distributed': 0.035617535939241995, 'district': 0.0, 'diversify': 0.0, 'diversion': 0.03983070069524599, 'diverting': 0.0, 'dlamini': 0.0, 'document': 0.0, 'documents': 0.028415083216529174, 'dollar': 0.0, 'dollars': 0.0, 'domestic': 0.0, 'dominance': 0.0, 'dominant': 0.0, 'dominate': 0.0, 'dominique': 0.0, 'done': 0.0, 'double': 0.0, 'downing': 0.0, 'dozen': 0.0, 'dprk': 0.0, 'drafting': 0.0, 'draws': 0.0, 'drilling': 0.0, 'driver': 0.0, 'dropped': 0.0, 'drug': 0.0, 'drugs': 0.0, 'dual': 0.0, 'duc': 0.0, 'due': 0.0, 'dutiable': 0.0, 'duty': 0.0, 'dynamics': 0.0, 'eager': 0.0, 'earlier': 0.0, 'early': 0.0, 'east': 0.0, 'eastern': 0.0, 'easy': 0.0, 'economic': 0.0, 'economics': 0.0, 'economy': 0.0, 'ecuador': 0.0, 'editorial': 0.0, 'educated': 0.0, 'education': 0.0, 'effect': 0.0, 'effective': 0.0, 'effectively': 0.0, 'efficiency': 0.0, 'efficient': 0.0, 'effort': 0.03262824797253317, 'efforts': 0.023107123366281113, 'el': 0.0, 'elect': 0.0, 'elected': 0.0, 'election': 0.0, 'elections': 0.0, 'electric': 0.0, 'electrical': 0.0, 'electricity': 0.0, 'electron': 0.0, 'elsewhere': 0.0, 'embargo': 0.0, 'embassy': 0.0, 'embezzlement': 0.0, 'emerged': 0.0, 'emergency': 0.0, 'emirates': 0.0, 'emphasized': 0.03983070069524599, 'empire': 0.0, 'employ': 0.0, 'employees': 0.0, 'employs': 0.0, 'en': 0.0, 'encircling': 0.0, 'enclosures': 0.0, 'encountered': 0.0, 'encourage': 0.0, 'encouraged': 0.0, 'encourages': 0.0, 'end': 0.0, 'ended': 0.0, 'endorsed': 0.0, 'energy': 0.06932137009884334, 'enforce': 0.0, 'enforced': 0.0, 'enforcement': 0.0, 'engage': 0.0, 'engaged': 0.0, 'engaging': 0.0, 'engine': 0.0, 'engineering': 0.0, 'english': 0.0, 'enhance': 0.0, 'enhanced': 0.0, 'enhancing': 0.0, 'enlargement': 0.0, 'enlightenment': 0.0, 'enlisted': 0.0, 'enormous': 0.0, 'enough': 0.0, 'enrich': 0.03983070069524599, 'enriched': 0.0, 'enrichment': 0.03262824797253317, 'ensure': 0.0, 'enter': 0.0, 'entered': 0.0, 'enterprises': 0.0, 'environment': 0.0, 'envoys': 0.0, 'equality': 0.0, 'equip': 0.0, 'equipment': 0.0, 'era': 0.0, 'eradicate': 0.0, 'erased': 0.0, 'escalated': 0.0, 'escape': 0.0, 'especially': 0.0, 'espinoza': 0.0, 'essential': 0.0, 'establish': 0.0, 'established': 0.0, 'establishment': 0.0, 'estacio': 0.0, 'estado': 0.0, 'estimate': 0.0, 'estimated': 0.0, 'estimates': 0.0, 'ethical': 0.0, 'eu': 0.0, 'europe': 0.0, 'european': 0.05085159049964069, 'evaded': 0.0, 'even': 0.0, 'evening': 0.0, 'event': 0.030309576088993935, 'events': 0.0, 'eventually': 0.0, 'ever': 0.0, 'every': 0.0, 'evidence': 0.030309576088993935, 'evin': 0.0, 'ex': 0.0, 'exaggeration': 0.0, 'examine': 0.0, 'example': 0.0, 'exceeded': 0.0, 'exceeding': 0.0, 'exceeds': 0.0, 'excessive': 0.0, 'exchange': 0.0, 'exchanged': 0.0, 'exchanges': 0.0, 'exclusively': 0.0, 'execution': 0.0, 'executive': 0.035617535939241995, 'exemption': 0.0, 'exercise': 0.0, 'exercises': 0.0, 'exile': 0.0, 'existing': 0.0, 'exists': 0.03983070069524599, 'expand': 0.0, 'expanded': 0.0, 'expanding': 0.0, 'expansion': 0.0, 'expansive': 0.0, 'expected': 0.0, 'experience': 0.0, 'experiments': 0.0, 'expert': 0.0, 'expertise': 0.0, 'experts': 0.1340665615798189, 'expired': 0.0, 'explanation': 0.0, 'exploded': 0.0, 'exploiting': 0.0, 'explosion': 0.0, 'explosions': 0.0, 'export': 0.0, 'exports': 0.0, 'expose': 0.0, 'exposing': 0.0, 'expressed': 0.0, 'extension': 0.0, 'extensive': 0.0, 'extent': 0.0, 'external': 0.0, 'extrajudicially': 0.0, 'extreme': 0.0, 'extremism': 0.0, 'extremists': 0.0, 'faces': 0.0, 'facilitate': 0.0, 'facilities': 0.030309576088993935, 'facility': 0.0, 'facing': 0.0, 'fact': 0.03262824797253317, 'facto': 0.0, 'factor': 0.0, 'factory': 0.0, 'failed': 0.0, 'fails': 0.0, 'failure': 0.0, 'fair': 0.0, 'fal': 0.0, 'falls': 0.03983070069524599, 'familiar': 0.0, 'famously': 0.0, 'far': 0.0, 'farc': 0.0, 'farmer': 0.0, 'farmers': 0.0, 'fars': 0.035617535939241995, 'fast': 0.0, 'fatah': 0.0, 'fatwa': 0.0, 'favor': 0.0, 'fear': 0.0, 'february': 0.0, 'federal': 0.0, 'feel': 0.0, 'feeling': 0.0, 'feels': 0.0, 'fell': 0.0, 'fellow': 0.0, 'felt': 0.0, 'fernandes': 0.0, 'fernandinho': 0.0, 'fernando': 0.0, 'fewer': 0.0, 'fidel': 0.0, 'field': 0.0, 'fields': 0.0, 'fifth': 0.0, 'fight': 0.0, 'fighting': 0.0, 'figures': 0.0, 'file': 0.0, 'files': 0.03983070069524599, 'filling': 0.0, 'final': 0.0, 'finance': 0.0, 'financed': 0.0, 'finances': 0.0, 'financial': 0.0, 'financing': 0.0, 'find': 0.0, 'findings': 0.03983070069524599, 'finds': 0.0, 'fine': 0.0, 'finished': 0.0, 'finland': 0.0, 'fire': 0.0, 'firepower': 0.0, 'firing': 0.0, 'firm': 0.0, 'firmly': 0.0, 'firms': 0.0, 'first': 0.0, 'fiscal': 0.0, 'fissile': 0.0, 'fittings': 0.0, 'fitzpatrick': 0.0, 'fixed': 0.0, 'flanges': 0.0, 'fled': 0.0, 'fledged': 0.0, 'flew': 0.0, 'flexible': 0.0, 'flight': 0.0, 'flights': 0.0, 'flow': 0.0, 'flying': 0.0, 'focus': 0.0, 'focused': 0.0, 'foiled': 0.0, 'follow': 0.0, 'following': 0.0, 'follows': 0.0, 'foment': 0.0, 'food': 0.0, 'forbid': 0.0, 'force': 0.0, 'forced': 0.0, 'forces': 0.0, 'foreign': 0.0, 'foreigners': 0.0, 'forerunner': 0.0, 'formed': 0.0, 'former': 0.0, 'forms': 0.0, 'fort': 0.0, 'fortaleza': 0.0, 'forum': 0.0, 'forward': 0.09788474391759949, 'found': 0.0, 'foundation': 0.0, 'founded': 0.0, 'frames': 0.0, 'framework': 0.0, 'france': 0.0, 'frankfurt': 0.0, 'free': 0.0, 'freed': 0.0, 'freeze': 0.0, 'freighter': 0.0, 'french': 0.0, 'friendship': 0.0, 'front': 0.0, 'frozen': 0.0, 'fruitful': 0.0, 'fuel': 0.0, 'fujimori': 0.0, 'full': 0.0, 'function': 0.0, 'functionality': 0.0, 'fundamental': 0.03983070069524599, 'funded': 0.0, 'funding': 0.0, 'funds': 0.0, 'furnaces': 0.0, 'future': 0.03262824797253317, 'gain': 0.0, 'game': 0.0, 'gandra': 0.0, 'gang': 0.0, 'gangs': 0.0, 'garrido': 0.0, 'gas': 0.0, 'gather': 0.0, 'gathered': 0.0, 'gen': 0.0, 'general': 0.0, 'generals': 0.0, 'generic': 0.0, 'geneva': 0.0, 'george': 0.0, 'georgia': 0.0, 'georgian': 0.0, 'gerhard': 0.0, 'german': 0.0, 'germany': 0.0, 'germs': 0.0, 'gestures': 0.0, 'give': 0.0, 'given': 0.0, 'gives': 0.03262824797253317, 'global': 0.0, 'gloria': 0.0, 'glut': 0.0, 'goal': 0.0, 'going': 0.0, 'gold': 0.0, 'good': 0.0, 'gottfried': 0.0, 'government': 0.04203053331331061, 'governmental': 0.0, 'governments': 0.0, 'grade': 0.0, 'grand': 0.0, 'granted': 0.0, 'granting': 0.0, 'grassroots': 0.0, 'great': 0.0, 'greatest': 0.0, 'greenwich': 0.0, 'greeted': 0.0, 'grenade': 0.0, 'grossly': 0.0, 'ground': 0.0, 'group': 0.0, 'groups': 0.0, 'growing': 0.0, 'growth': 0.0, 'guangdong': 0.0, 'guarantee': 0.0, 'guerrilla': 0.0, 'guerrillas': 0.0, 'guidelines': 0.0, 'gulf': 0.0, 'gun': 0.0, 'guns': 0.0, 'hacking': 0.0, 'hadre': 0.0, 'haggling': 0.0, 'half': 0.0, 'halt': 0.03262824797253317, 'halts': 0.0, 'hambali': 0.0, 'hanau': 0.0, 'handed': 0.0, 'handling': 0.0, 'hanoi': 0.0, 'hardware': 0.0, 'head': 0.0, 'headline': 0.0, 'headquarters': 0.0, 'health': 0.0, 'healthcare': 0.0, 'healthy': 0.0, 'heard': 0.0, 'heart': 0.0, 'heavily': 0.0, 'heavy': 0.0, 'heightened': 0.0, 'held': 0.0, 'helicopter': 0.0, 'helicopters': 0.0, 'henrique': 0.0, 'heroin': 0.0, 'hesitating': 0.0, 'hi': 0.0, 'hidden': 0.0, 'high': 0.0, 'higher': 0.0, 'highest': 0.0, 'highly': 0.0, 'hijacked': 0.0, 'hill': 0.0, 'himalayan': 0.0, 'hinder': 0.0, 'hindered': 0.0, 'hiro': 0.0, 'historic': 0.0, 'history': 0.0, 'hkust': 0.0, 'hold': 0.0, 'holding': 0.0, 'holy': 0.0, 'home': 0.0, 'hong': 0.0, 'hope': 0.0, 'hopes': 0.0, 'hospital': 0.0, 'host': 0.0, 'hostage': 0.0, 'hour': 0.0, 'howard': 0.0, 'huallaga': 0.0, 'huge': 0.0, 'hugo': 0.0, 'human': 0.0, 'hundreds': 0.0, 'hyun': 0.0, 'iaea': 0.35617535939241995, 'ibn': 0.0, 'idea': 0.035617535939241995, 'idec': 0.0, 'identified': 0.0, 'ideological': 0.0, 'illegal': 0.0, 'illegally': 0.0, 'illegitimate': 0.0, 'image': 0.0, 'imagers': 0.0, 'images': 0.0, 'immediate': 0.0, 'immediately': 0.0, 'immigration': 0.0, 'immigrations': 0.0, 'imminent': 0.0, 'immune': 0.0, 'immunity': 0.0, 'impact': 0.0, 'imperative': 0.0, 'imperialist': 0.0, 'implement': 0.0, 'implementation': 0.0, 'implemented': 0.0, 'implication': 0.0, 'importance': 0.0, 'important': 0.0, 'impose': 0.07123507187848399, 'imposed': 0.0, 'imposing': 0.0, 'imposition': 0.0, 'imprisoned': 0.0, 'imprisonment': 0.0, 'improved': 0.0, 'improving': 0.0, 'inaccurate': 0.0, 'inacio': 0.0, 'inapplicable': 0.0, 'inaugurated': 0.0, 'incapable': 0.0, 'incentives': 0.0, 'incident': 0.0, 'incidents': 0.0, 'include': 0.0, 'included': 0.0, 'includes': 0.0, 'including': 0.02211676073726579, 'incomes': 0.0, 'incoming': 0.0, 'incomplete': 0.0, 'inconclusive': 0.0, 'incorrect': 0.0, 'increase': 0.0, 'increased': 0.0, 'increasing': 0.0, 'increasingly': 0.0, 'incursion': 0.0, 'independent': 0.035617535939241995, 'independently': 0.0, 'india': 0.0, 'indian': 0.0, 'indicated': 0.0, 'indicates': 0.0, 'indicating': 0.03983070069524599, 'indication': 0.0, 'individual': 0.0, 'indoctrinate': 0.0, 'indoctrination': 0.0, 'indonesia': 0.0, 'industrial': 0.0, 'industrialization': 0.0, 'industries': 0.0, 'industry': 0.0, 'infiltrate': 0.0, 'infiltrated': 0.0, 'influence': 0.0, 'info': 0.0, 'information': 0.028415083216529174, 'infrastructure': 0.0, 'inherited': 0.0, 'initially': 0.0, 'initiated': 0.0, 'initiating': 0.0, 'initiative': 0.0, 'initiatives': 0.0, 'injured': 0.0, 'injuring': 0.0, 'innocent': 0.0, 'inside': 0.0, 'insight': 0.03983070069524599, 'insisted': 0.0, 'insists': 0.0, 'inspected': 0.0, 'inspection': 0.0, 'inspector': 0.0, 'inspectors': 0.06525649594506634, 'inspired': 0.0, 'installation': 0.0, 'installations': 0.0, 'instead': 0.0, 'instigate': 0.0, 'institute': 0.028415083216529174, 'institutes': 0.0, 'institution': 0.0, 'institutions': 0.0, 'insult': 0.0, 'insurance': 0.0, 'insurgencies': 0.0, 'insurgency': 0.0, 'insurgent': 0.0, 'insurgents': 0.0, 'intelligence': 0.0, 'intends': 0.0, 'intense': 0.0, 'intensity': 0.0, 'intention': 0.0, 'intercept': 0.0, 'intercepting': 0.0, 'intercontinental': 0.0, 'intercourse': 0.0, 'interest': 0.0, 'interested': 0.0, 'interests': 0.0, 'interfax': 0.0, 'interim': 0.0, 'intermittently': 0.0, 'internal': 0.0, 'international': 0.10556037328804693, 'internationally': 0.0, 'internet': 0.0, 'intervention': 0.035617535939241995, 'interventions': 0.0, 'interview': 0.026813312315963782, 'interviewed': 0.0, 'intimated': 0.0, 'introducing': 0.0, 'invade': 0.0, 'invaded': 0.0, 'invasion': 0.0, 'invest': 0.0, 'investigation': 0.0, 'investigations': 0.0, 'investing': 0.0, 'investment': 0.0, 'invited': 0.0, 'involve': 0.0, 'involved': 0.0, 'involves': 0.0, 'involving': 0.0, 'ip': 0.0, 'iran': 0.25417835702909225, 'iranian': 0.30510954299784415, 'iranians': 0.0, 'iraq': 0.0, 'iraqi': 0.0, 'irregular': 0.0, 'irrelevant': 0.03983070069524599, 'irrigated': 0.0, 'isamudin': 0.0, 'islam': 0.0, 'islamic': 0.0, 'islamist': 0.0, 'isolated': 0.0, 'israel': 0.0, 'israeli': 0.0, 'issue': 0.030309576088993935, 'issued': 0.0, 'issues': 0.05085159049964069, 'italian': 0.0, 'items': 0.0, 'ivanov': 0.0, 'izvestia': 0.0, 'jackets': 0.0, 'jacqueline': 0.03983070069524599, 'jacques': 0.0, 'janeiro': 0.0, 'january': 0.0, 'japan': 0.0, 'japanese': 0.0, 'javier': 0.0, 'jeffrey': 0.0, 'jet': 0.0, 'jiabao': 0.0, 'jiaxuan': 0.0, 'johannesburg': 0.0, 'john': 0.0, 'joined': 0.0, 'joint': 0.0, 'jointly': 0.0, 'jose': 0.0, 'judge': 0.0, 'judicial': 0.0, 'juice': 0.0, 'julio': 0.0, 'july': 0.0, 'june': 0.0, 'junichiro': 0.0, 'juridical': 0.0, 'justice': 0.0, 'kailali': 0.0, 'kamchatka': 0.0, 'karl': 0.0, 'kashmir': 0.0, 'kathmandu': 0.0, 'kazakhstan': 0.0, 'keeps': 0.0, 'kenya': 0.0, 'kenyan': 0.0, 'key': 0.03262824797253317, 'khai': 0.0, 'khalid': 0.0, 'kidnapped': 0.0, 'killed': 0.0, 'kilometers': 0.0, 'kimball': 0.07123507187848399, 'kind': 0.0, 'kindi': 0.0, 'kingdom': 0.0, 'km': 0.0, 'know': 0.0, 'known': 0.0, 'koizumi': 0.0, 'kong': 0.0, 'korea': 0.0, 'korean': 0.0, 'kowloon': 0.0, 'krasnoznamernsk': 0.0, 'kwong': 0.0, 'kyrgyz': 0.0, 'kyrgyzstan': 0.0, 'labeled': 0.0, 'laboratory': 0.0, 'lack': 0.0, 'laden': 0.0, 'ladjevardi': 0.0, 'lalitpur': 0.0, 'land': 0.0, 'landed': 0.0, 'language': 0.0, 'large': 0.0, 'largely': 0.0, 'largest': 0.0, 'lasers': 0.0, 'last': 0.0, 'lasted': 0.0, 'late': 0.0, 'lateralism': 0.0, 'latest': 0.028415083216529174, 'latin': 0.0, 'lau': 0.0, 'launch': 0.0, 'launched': 0.0, 'launchers': 0.0, 'launches': 0.0, 'laundered': 0.0, 'laundering': 0.0, 'law': 0.0, 'laws': 0.0, 'lawsuits': 0.0, 'lawyer': 0.0, 'layered': 0.0, 'lead': 0.035617535939241995, 'leader': 0.0, 'leaders': 0.0, 'leadership': 0.0, 'leading': 0.0, 'leaked': 0.0, 'learn': 0.0, 'learned': 0.0, 'least': 0.0, 'leave': 0.0, 'led': 0.0, 'lee': 0.0, 'left': 0.0, 'leftist': 0.0, 'leftists': 0.0, 'leftward': 0.0, 'legacy': 0.0, 'legal': 0.0, 'leibnitz': 0.0, 'leon': 0.0, 'less': 0.0, 'letter': 0.0, 'level': 0.0, 'leverage': 0.0, 'lewis': 0.0, 'licensed': 0.0, 'lieutenant': 0.0, 'life': 0.0, 'lift': 0.0, 'like': 0.0, 'likely': 0.030309576088993935, 'likened': 0.0, 'lima': 0.0, 'limitation': 0.0, 'limited': 0.07123507187848399, 'line': 0.0, 'lines': 0.0, 'linked': 0.0, 'linking': 0.0, 'links': 0.0, 'liquid': 0.0, 'list': 0.0, 'listed': 0.0, 'lithium': 0.0, 'little': 0.03983070069524599, 'live': 0.0, 'loan': 0.0, 'loans': 0.0, 'local': 0.0, 'located': 0.0, 'location': 0.0, 'locations': 0.0, 'locomotives': 0.0, 'logistics': 0.0, 'london': 0.035617535939241995, 'long': 0.0, 'longtime': 0.0, 'looking': 0.0, 'loosing': 0.0, 'lords': 0.0, 'lorenzo': 0.0, 'lose': 0.0, 'loses': 0.0, 'loss': 0.0, 'lost': 0.0, 'lot': 0.0, 'low': 0.0, 'loyal': 0.0, 'loyalists': 0.0, 'lt': 0.0, 'luiz': 0.0, 'lula': 0.0, 'luong': 0.0, 'macao': 0.0, 'macapagal': 0.0, 'machines': 0.0, 'macro': 0.0, 'made': 0.02211676073726579, 'madrid': 0.0, 'maher': 0.0, 'mahmoud': 0.0, 'mail': 0.0, 'main': 0.0, 'mainland': 0.0, 'mainstream': 0.0, 'maintain': 0.0, 'maintained': 0.0, 'maintaining': 0.0, 'major': 0.0, 'majority': 0.0, 'make': 0.025425795249820346, 'makes': 0.0, 'making': 0.0, 'manage': 0.0, 'managed': 0.0, 'management': 0.0, 'maneuvering': 0.0, 'manila': 0.0, 'manner': 0.0, 'manufacture': 0.0, 'manufacturer': 0.0, 'manufacturers': 0.0, 'manufacturing': 0.0, 'manusha': 0.0, 'many': 0.025425795249820346, 'map': 0.0, 'mar': 0.0, 'march': 0.0, 'marched': 0.0, 'maria': 0.0, 'marie': 0.0, 'marines': 0.0, 'maritime': 0.0, 'mark': 0.0, 'marked': 0.0, 'market': 0.0, 'markets': 0.0, 'markings': 0.0, 'married': 0.0, 'marx': 0.0, 'mashbad': 0.0, 'masks': 0.03983070069524599, 'mass': 0.0, 'massive': 0.0, 'mastering': 0.0, 'mastermind': 0.0, 'masterminded': 0.0, 'material': 0.026813312315963782, 'materials': 0.035617535939241995, 'matter': 0.0, 'matters': 0.0, 'mauro': 0.0, 'may': 0.04242526098763271, 'mbeki': 0.0, 'mean': 0.0, 'means': 0.0, 'meant': 0.0, 'measure': 0.0, 'measures': 0.028415083216529174, 'mechanical': 0.0, 'mechanism': 0.0, 'media': 0.0, 'medium': 0.0, 'meet': 0.0, 'meeting': 0.023107123366281113, 'meetings': 0.0, 'meets': 0.0, 'member': 0.0, 'members': 0.02211676073726579, 'membership': 0.0, 'men': 0.0, 'menace': 0.0, 'mentions': 0.0, 'mercenary': 0.0, 'mess': 0.0, 'met': 0.035617535939241995, 'method': 0.0, 'methods': 0.0, 'metro': 0.0, 'metropolitan': 0.0, 'michele': 0.0, 'microbiology': 0.0, 'microscopes': 0.0, 'mid': 0.0, 'middle': 0.0, 'might': 0.0, 'mikheil': 0.0, 'miles': 0.0, 'militaries': 0.0, 'militarily': 0.0, 'militarized': 0.0, 'military': 0.0, 'militias': 0.0, 'million': 0.0, 'millions': 0.0, 'mind': 0.0, 'minded': 0.0, 'mineral': 0.0, 'mines': 0.0, 'minister': 0.0, 'ministers': 0.0, 'ministry': 0.0, 'minus': 0.0, 'mirage': 0.0, 'miranda': 0.0, 'misidentified': 0.0, 'misleading': 0.0, 'missile': 0.0, 'missiles': 0.0, 'missing': 0.0, 'mission': 0.0, 'missionary': 0.0, 'mixed': 0.0, 'mobilized': 0.0, 'modern': 0.0, 'modernize': 0.0, 'modes': 0.0, 'modification': 0.0, 'mohammed': 0.0, 'mombasa': 0.0, 'monetary': 0.0, 'money': 0.0, 'mongkok': 0.0, 'monitor': 0.0, 'monitored': 0.0, 'monitoring': 0.0, 'monitors': 0.0, 'monterey': 0.0, 'montesinos': 0.0, 'month': 0.0, 'months': 0.0, 'morning': 0.0, 'moscow': 0.0, 'mosenergo': 0.0, 'mostly': 0.0, 'mosul': 0.0, 'motivated': 0.0, 'mountainous': 0.0, 'move': 0.035617535939241995, 'movement': 0.0, 'movements': 0.0, 'moves': 0.0, 'moving': 0.035617535939241995, 'mr': 0.0, 'much': 0.028415083216529174, 'mujahedeen': 0.0, 'multi': 0.0, 'multicultural': 0.0, 'multilateral': 0.0, 'multimedia': 0.0, 'municipality': 0.0, 'munitions': 0.0, 'murdered': 0.0, 'muslim': 0.0, 'muslims': 0.0, 'must': 0.026813312315963782, 'mutual': 0.0, 'mutually': 0.0, 'myanmar': 0.0, 'najaf': 0.0, 'named': 0.0, 'names': 0.0, 'nanomaterial': 0.0, 'nanotechnology': 0.0, 'narcotics': 0.0, 'natanz': 0.0, 'nation': 0.0, 'national': 0.0, 'nationality': 0.0, 'nationals': 0.0, 'nations': 0.0, 'nationwide': 0.0, 'nato': 0.0, 'nature': 0.035617535939241995, 'naval': 0.0, 'navy': 0.0, 'near': 0.0, 'nearly': 0.0, 'necessary': 0.0, 'necessity': 0.0, 'need': 0.0, 'needed': 0.03983070069524599, 'negative': 0.0, 'negotiate': 0.0, 'negotiating': 0.0, 'negotiation': 0.0, 'negotiations': 0.0, 'negotiator': 0.0, 'negotiators': 0.0, 'neighborhoods': 0.0, 'neighbors': 0.0, 'neither': 0.0, 'nepal': 0.0, 'nepalgunj': 0.0, 'nepali': 0.0, 'netting': 0.0, 'network': 0.0, 'networks': 0.0, 'never': 0.0, 'new': 0.02038091129479884, 'news': 0.05085159049964069, 'newspaper': 0.0, 'next': 0.0, 'nickel': 0.0, 'nightclub': 0.0, 'nikolai': 0.0, 'nine': 0.0, 'nitrogen': 0.0, 'non': 0.0, 'nonpaper': 0.0, 'nonpartisan': 0.03983070069524599, 'nonproliferation': 0.06525649594506634, 'nonsense': 0.0, 'nordic': 0.0, 'normally': 0.0, 'norms': 0.0, 'north': 0.0, 'northeast': 0.0, 'northeastern': 0.0, 'northern': 0.0, 'northwest': 0.0, 'notice': 0.0, 'notify': 0.0, 'notifying': 0.0, 'notion': 0.0, 'notorious': 0.0, 'november': 0.0, 'nsg': 0.0, 'nuclear': 0.23107123366281113, 'number': 0.0, 'numerous': 0.0, 'obama': 0.0, 'obligations': 0.03983070069524599, 'obolensk': 0.0, 'obscene': 0.0, 'observe': 0.0, 'obsolete': 0.0, 'obstacles': 0.0, 'obtain': 0.03983070069524599, 'occupied': 0.0, 'occur': 0.0, 'occurred': 0.0, 'ochoa': 0.0, 'october': 0.0, 'offensives': 0.0, 'offer': 0.035617535939241995, 'offered': 0.0, 'offers': 0.0, 'office': 0.0, 'officer': 0.0, 'officers': 0.0, 'official': 0.023107123366281113, 'officially': 0.0, 'officials': 0.16304729035839072, 'offline': 0.0, 'offshoot': 0.0, 'often': 0.0, 'oil': 0.0, 'okubahal': 0.0, 'old': 0.0, 'oleh': 0.0, 'omar': 0.0, 'one': 0.0, 'open': 0.0, 'opening': 0.0, 'openly': 0.0, 'openness': 0.0, 'operates': 0.0, 'operating': 0.0, 'operation': 0.0, 'operational': 0.0, 'operations': 0.0, 'opined': 0.0, 'opinions': 0.0, 'opium': 0.0, 'opponents': 0.0, 'opportunities': 0.0, 'opposed': 0.0, 'opposition': 0.0, 'optimal': 0.0, 'optimistic': 0.0, 'orchestrate': 0.0, 'order': 0.0, 'ordered': 0.0, 'orders': 0.0, 'ore': 0.0, 'organ': 0.0, 'organisms': 0.0, 'organization': 0.025425795249820346, 'organizations': 0.0, 'organized': 0.0, 'oriental': 0.0, 'origin': 0.0, 'original': 0.0, 'originally': 0.0, 'osama': 0.0, 'ossetia': 0.0, 'others': 0.0, 'oust': 0.0, 'outdated': 0.0, 'outlined': 0.0, 'outlining': 0.0, 'output': 0.0, 'outside': 0.0, 'overdue': 0.0, 'oversize': 0.0, 'overwhelmingly': 0.0, 'owners': 0.0, 'ownership': 0.0, 'package': 0.0, 'pact': 0.0, 'page': 0.0, 'paid': 0.0, 'pakistan': 0.0, 'palestine': 0.0, 'paolo': 0.0, 'paperwork': 0.0, 'paris': 0.03262824797253317, 'park': 0.0, 'parliamentary': 0.0, 'part': 0.0, 'participate': 0.0, 'participated': 0.0, 'participating': 0.0, 'particularly': 0.0, 'parties': 0.0, 'partner': 0.0, 'partnering': 0.0, 'partners': 0.0, 'partnership': 0.0, 'party': 0.0, 'pass': 0.0, 'passengers': 0.0, 'passive': 0.0, 'past': 0.0, 'path': 0.035617535939241995, 'pathogens': 0.0, 'paul': 0.0, 'pay': 0.0, 'paying': 0.0, 'payment': 0.0, 'peace': 0.0, 'peaceful': 0.0, 'peacefully': 0.0, 'peaked': 0.0, 'pearl': 0.0, 'pena': 0.0, 'penalties': 0.03983070069524599, 'penalty': 0.0, 'peninsula': 0.0, 'pentagon': 0.0, 'people': 0.021212630493816356, 'per': 0.0, 'perceived': 0.0, 'percent': 0.0, 'perhaps': 0.0, 'period': 0.0, 'permanent': 0.0, 'permission': 0.0, 'permit': 0.0, 'permitted': 0.0, 'persian': 0.0, 'person': 0.0, 'personnel': 0.0, 'persons': 0.0, 'persuade': 0.0, 'peru': 0.0, 'peruvian': 0.0, 'petrochemical': 0.0, 'petroleum': 0.0, 'phan': 0.0, 'phase': 0.0, 'philippine': 0.0, 'philippines': 0.0, 'philosophers': 0.0, 'philosophy': 0.0, 'phone': 0.0, 'physical': 0.0, 'physicist': 0.0, 'physics': 0.0, 'pillay': 0.0, 'pilots': 0.0, 'pipeline': 0.0, 'pirate': 0.0, 'pirated': 0.0, 'pirates': 0.0, 'place': 0.0, 'placed': 0.0, 'plague': 0.0, 'plan': 0.024201918460525183, 'plane': 0.0, 'planes': 0.0, 'planned': 0.0, 'planning': 0.0, 'plans': 0.0, 'plant': 0.0, 'plantations': 0.0, 'planted': 0.0, 'plasma': 0.0, 'play': 0.0, 'pledged': 0.0, 'plenty': 0.0, 'plot': 0.0, 'plutonium': 0.0, 'points': 0.0, 'polar': 0.0, 'pole': 0.0, 'police': 0.0, 'policemen': 0.0, 'policies': 0.0, 'policy': 0.0, 'policymakers': 0.03983070069524599, 'political': 0.0, 'politicians': 0.0, 'politics': 0.0, 'poll': 0.0, 'polling': 0.0, 'polytechnic': 0.0, 'polyu': 0.0, 'poon': 0.0, 'popovkin': 0.0, 'poppy': 0.0, 'popular': 0.0, 'popularity': 0.0, 'population': 0.0, 'port': 0.0, 'pose': 0.0, 'posed': 0.0, 'positions': 0.0, 'positive': 0.0, 'positively': 0.0, 'possessed': 0.0, 'possession': 0.0, 'possibility': 0.0, 'possible': 0.028415083216529174, 'possibly': 0.03983070069524599, 'post': 0.0, 'posted': 0.0, 'posts': 0.0, 'potential': 0.0, 'potentially': 0.035617535939241995, 'poti': 0.0, 'poverty': 0.0, 'powder': 0.0, 'power': 0.0, 'powerful': 0.0, 'powerfully': 0.0, 'practical': 0.0, 'pre': 0.0, 'precautionary': 0.0, 'precedent': 0.0, 'precursor': 0.0, 'preferably': 0.0, 'premier': 0.0, 'preparation': 0.0, 'preparations': 0.0, 'prepared': 0.0, 'preparing': 0.0, 'presence': 0.0, 'present': 0.03983070069524599, 'presentation': 0.0, 'presented': 0.0, 'preserve': 0.0, 'preserved': 0.0, 'president': 0.0, 'presidential': 0.0, 'press': 0.0, 'pressing': 0.0, 'pressure': 0.05683016643305835, 'pretoria': 0.0, 'prevent': 0.03983070069524599, 'prevented': 0.0, 'previous': 0.0, 'previously': 0.028415083216529174, 'price': 0.0, 'prices': 0.0, 'primarily': 0.0, 'prime': 0.0, 'principle': 0.0, 'principles': 0.030309576088993935, 'prior': 0.0, 'priorities': 0.0, 'priority': 0.0, 'prison': 0.0, 'private': 0.0, 'pro': 0.0, 'probably': 0.0, 'problem': 0.03262824797253317, 'problems': 0.0, 'proceed': 0.0, 'process': 0.0, 'processed': 0.0, 'produce': 0.0, 'produced': 0.0, 'producing': 0.0, 'production': 0.08043993694789134, 'products': 0.0, 'professional': 0.0, 'profiled': 0.0, 'profits': 0.0, 'profound': 0.0, 'program': 0.17798056674874244, 'programs': 0.0, 'progress': 0.028415083216529174, 'progressed': 0.0, 'prohibited': 0.0, 'prohibition': 0.0, 'prohibitions': 0.0, 'prohibits': 0.0, 'projects': 0.0, 'proliferation': 0.0, 'prominent': 0.0, 'promise': 0.0, 'promote': 0.0, 'promoted': 0.0, 'promoting': 0.0, 'prompted': 0.0, 'proof': 0.0, 'propelled': 0.0, 'propitiation': 0.0, 'proposal': 0.0, 'proposals': 0.0, 'proposed': 0.0, 'prosecuted': 0.0, 'protect': 0.0, 'protection': 0.0, 'provide': 0.03262824797253317, 'provided': 0.0, 'province': 0.0, 'provinces': 0.0, 'provision': 0.0, 'provisions': 0.0, 'proxy': 0.0, 'psychological': 0.0, 'public': 0.0, 'publicity': 0.0, 'publicizing': 0.03983070069524599, 'publicly': 0.0, 'published': 0.03262824797253317, 'publisher': 0.0, 'punished': 0.0, 'punishment': 0.0, 'purchase': 0.0, 'purchased': 0.0, 'purchases': 0.0, 'purchasing': 0.0, 'purely': 0.0, 'puritanical': 0.0, 'purpose': 0.0, 'purposes': 0.0, 'pursued': 0.0, 'pursuing': 0.030309576088993935, 'put': 0.0, 'putin': 0.0, 'putting': 0.0, 'pyongyang': 0.0, 'qa': 0.0, 'qaa': 0.0, 'qaeda': 0.0, 'qaib': 0.0, 'qualify': 0.0, 'quality': 0.0, 'quantity': 0.0, 'question': 0.0, 'questioned': 0.0, 'questioning': 0.0, 'questions': 0.035617535939241995, 'quickly': 0.0, 'quintero': 0.0, 'quite': 0.0, 'quoted': 0.028415083216529174, 'radar': 0.0, 'radical': 0.0, 'radio': 0.0, 'radiometric': 0.0, 'raduan': 0.0, 'rafidha': 0.0, 'raided': 0.0, 'raids': 0.0, 'railway': 0.0, 'raises': 0.0, 'ramon': 0.0, 'ranchers': 0.0, 'randall': 0.0, 'range': 0.0, 'ranking': 0.0, 'ransom': 0.0, 'rapidly': 0.0, 'rather': 0.0, 'rating': 0.0, 'rational': 0.0, 'raw': 0.0, 'raymond': 0.0, 'reaccelerated': 0.0, 'reach': 0.0, 'reached': 0.0, 'reaching': 0.0, 'reaction': 0.0, 'reactor': 0.0, 'reactors': 0.0, 'ready': 0.0, 'reaffirmed': 0.0, 'real': 0.0, 'reality': 0.0, 'realize': 0.0, 'realizing': 0.0, 'reason': 0.0, 'rebel': 0.0, 'rebels': 0.0, 'receipt': 0.0, 'receive': 0.0, 'received': 0.0, 'recent': 0.0, 'recently': 0.0, 'recognized': 0.0, 'reconstruction': 0.0, 'record': 0.0, 'recorded': 0.0, 'recordings': 0.0, 'records': 0.0, 'recoup': 0.0, 'recruitment': 0.0, 'reduced': 0.0, 'reducing': 0.0, 'reduction': 0.03262824797253317, 'referenced': 0.0, 'referred': 0.0, 'refined': 0.0, 'refining': 0.0, 'reflect': 0.035617535939241995, 'reform': 0.0, 'reforms': 0.0, 'refrain': 0.0, 'refused': 0.0, 'refuted': 0.0, 'regard': 0.0, 'regarding': 0.07123507187848399, 'regime': 0.03262824797253317, 'region': 0.0, 'regional': 0.0, 'regions': 0.0, 'regulations': 0.0, 'rejected': 0.0, 'rejecters': 0.0, 'related': 0.0, 'relations': 0.0, 'relationship': 0.0, 'relationships': 0.0, 'relatively': 0.0, 'relatives': 0.0, 'release': 0.0, 'released': 0.0, 'relenting': 0.0, 'relevant': 0.0, 'reliable': 0.0, 'relied': 0.0, 'religious': 0.0, 'remained': 0.0, 'remaining': 0.03983070069524599, 'remains': 0.0, 'remnants': 0.0, 'removed': 0.0, 'render': 0.0, 'rendered': 0.0, 'renew': 0.0, 'renowned': 0.0, 'repeatedly': 0.0, 'repel': 0.0, 'report': 0.14521151076315109, 'reported': 0.0, 'reportedly': 0.030309576088993935, 'reporters': 0.0, 'reports': 0.0, 'repository': 0.0, 'representative': 0.03983070069524599, 'represented': 0.0, 'republic': 0.0, 'republican': 0.0, 'reputation': 0.0, 'request': 0.0, 'requested': 0.0, 'required': 0.0, 'requires': 0.0, 'research': 0.0, 'reserve': 0.0, 'reserves': 0.0, 'reservist': 0.0, 'reservists': 0.0, 'residences': 0.0, 'resignation': 0.0, 'resigned': 0.0, 'resigning': 0.0, 'resist': 0.0, 'resisted': 0.0, 'resolution': 0.0, 'resolutions': 0.0, 'resolve': 0.0, 'resolved': 0.06525649594506634, 'resolving': 0.0, 'resources': 0.0, 'respect': 0.0, 'respond': 0.0, 'responsibility': 0.0, 'responsible': 0.0, 'restored': 0.0, 'restrict': 0.0, 'restrictions': 0.0, 'restructure': 0.0, 'result': 0.0, 'resulted': 0.0, 'results': 0.0, 'resumption': 0.0, 'retaliation': 0.0, 'retired': 0.0, 'retreated': 0.0, 'retribution': 0.0, 'return': 0.0, 'returned': 0.0, 'returning': 0.0, 'reuters': 0.07966140139049198, 'revenue': 0.0, 'revolution': 0.0, 'revolutionary': 0.0, 'revolutions': 0.0, 'rfid': 0.0, 'rice': 0.0, 'ridiculing': 0.0, 'rifle': 0.0, 'rifles': 0.0, 'rig': 0.0, 'right': 0.0, 'rights': 0.0, 'rio': 0.0, 'rise': 0.0, 'risk': 0.0, 'river': 0.0, 'robbery': 0.0, 'rocket': 0.0, 'roh': 0.0, 'role': 0.0, 'rollback': 0.0, 'roque': 0.0, 'rose': 0.0, 'rotary': 0.0, 'round': 0.10685260781772599, 'rounds': 0.0, 'route': 0.0, 'routes': 0.0, 'royal': 0.0, 'rules': 0.0, 'ruling': 0.0, 'run': 0.0, 'running': 0.0, 'runs': 0.0, 'rural': 0.0, 'russia': 0.0, 'russian': 0.025425795249820346, 'ryabkov': 0.0, 'sa': 0.0, 'saakashvili': 0.0, 'safeguarding': 0.0, 'safeguards': 0.03983070069524599, 'said': 0.04423352147453158, 'sailors': 0.0, 'sale': 0.0, 'sales': 0.0, 'samoud': 0.0, 'sanctions': 0.15154788044496967, 'sao': 0.0, 'satellite': 0.0, 'satellites': 0.0, 'saudi': 0.0, 'saul': 0.0, 'say': 0.0, 'sayad': 0.0, 'saying': 0.0, 'says': 0.0, 'sayyaf': 0.0, 'scale': 0.0, 'scandal': 0.0, 'scared': 0.0, 'scheduled': 0.0, 'scholars': 0.0, 'schools': 0.0, 'schroeder': 0.0, 'science': 0.028415083216529174, 'sciences': 0.0, 'scientific': 0.0, 'scientist': 0.0, 'scientists': 0.0, 'sco': 0.0, 'scoff': 0.0, 'scope': 0.03983070069524599, 'scorpene': 0.0, 'scorpion': 0.0, 'sea': 0.0, 'sealing': 0.0, 'search': 0.0, 'seas': 0.0, 'season': 0.0, 'seat': 0.0, 'secession': 0.0, 'second': 0.0, 'secret': 0.0, 'secretariat': 0.0, 'secretary': 0.0, 'sectarian': 0.0, 'sectors': 0.0, 'securing': 0.0, 'security': 0.08499732868906182, 'see': 0.0, 'seeks': 0.0, 'seigel': 0.0, 'seized': 0.0, 'self': 0.0, 'selling': 0.0, 'senior': 0.030309576088993935, 'sense': 0.0, 'sensibly': 0.0, 'sent': 0.0, 'sentence': 0.0, 'sentenced': 0.0, 'separate': 0.0, 'separately': 0.0, 'separatism': 0.0, 'separatist': 0.0, 'separatists': 0.0, 'september': 0.06061915217798787, 'sergei': 0.0, 'series': 0.0, 'serious': 0.0, 'seriously': 0.0, 'served': 0.0, 'serves': 0.0, 'service': 0.0, 'services': 0.0, 'set': 0.0, 'sets': 0.0, 'setting': 0.0, 'settle': 0.0, 'settlement': 0.0, 'settling': 0.0, 'several': 0.0, 'sexual': 0.0, 'shame': 0.0, 'shanghai': 0.0, 'share': 0.0, 'shared': 0.0, 'sharing': 0.0, 'sheik': 0.0, 'shenzhen': 0.0, 'sher': 0.0, 'shia': 0.0, 'shields': 0.0, 'shift': 0.0, 'shifted': 0.0, 'shiite': 0.0, 'shiites': 0.0, 'shinning': 0.0, 'ship': 0.0, 'shipment': 0.0, 'shipped': 0.0, 'ships': 0.0, 'shire': 0.03983070069524599, 'shock': 0.0, 'shoot': 0.0, 'shooting': 0.0, 'shop': 0.0, 'shops': 0.0, 'short': 0.035617535939241995, 'shortly': 0.0, 'shot': 0.0, 'show': 0.0, 'showed': 0.0, 'shows': 0.035617535939241995, 'shut': 0.0, 'shutdown': 0.0, 'shutting': 0.0, 'siberia': 0.0, 'side': 0.0, 'siegel': 0.0, 'siemens': 0.0, 'sign': 0.035617535939241995, 'signatory': 0.0, 'signed': 0.0, 'significance': 0.0, 'significant': 0.030309576088993935, 'significantly': 0.0, 'signing': 0.0, 'signs': 0.0, 'silos': 0.0, 'silva': 0.0, 'similar': 0.0, 'simple': 0.0, 'since': 0.0, 'sincere': 0.0, 'singapore': 0.0, 'sino': 0.0, 'sistani': 0.0, 'site': 0.0, 'sites': 0.028415083216529174, 'situation': 0.0, 'sivam': 0.0, 'six': 0.0, 'sized': 0.0, 'slow': 0.0, 'slowed': 0.03983070069524599, 'small': 0.0, 'smashed': 0.0, 'smuggle': 0.0, 'smugglers': 0.0, 'smuggling': 0.0, 'social': 0.0, 'socialism': 0.0, 'solana': 0.0, 'sold': 0.0, 'soldier': 0.0, 'soldiers': 0.0, 'sole': 0.0, 'solid': 0.0, 'soltanieh': 0.07966140139049198, 'solution': 0.0, 'solved': 0.0, 'somali': 0.0, 'somalia': 0.0, 'soon': 0.0, 'sorting': 0.0, 'source': 0.0, 'sources': 0.0, 'south': 0.0, 'southern': 0.0, 'southwest': 0.0, 'sovereignty': 0.0, 'soviet': 0.0, 'space': 0.0, 'spain': 0.0, 'spanish': 0.0, 'speaking': 0.0, 'special': 0.0, 'specialist': 0.0, 'specialists': 0.0, 'specializing': 0.0, 'specific': 0.0, 'specify': 0.03983070069524599, 'spectrometer': 0.0, 'spend': 0.0, 'sphere': 0.0, 'spheres': 0.0, 'spirit': 0.0, 'spoke': 0.0, 'spokesman': 0.030309576088993935, 'spokeswoman': 0.0, 'spy': 0.0, 'stability': 0.0, 'stable': 0.0, 'staff': 0.0, 'stage': 0.0, 'staged': 0.0, 'stages': 0.0, 'stalled': 0.0, 'stance': 0.0, 'standoff': 0.0, 'start': 0.0, 'started': 0.0, 'starting': 0.0, 'state': 0.03778791722055424, 'stated': 0.17131996143711822, 'statement': 0.0, 'states': 0.0, 'stating': 0.026813312315963782, 'station': 0.0, 'stations': 0.0, 'status': 0.0, 'statute': 0.0, 'staunchest': 0.0, 'staying': 0.0, 'steel': 0.0, 'step': 0.030309576088993935, 'steps': 0.0, 'stern': 0.0, 'still': 0.0, 'stockpiled': 0.0, 'stockpiles': 0.0, 'stockpiling': 0.0, 'stolte': 0.0, 'stood': 0.0, 'stopping': 0.0, 'stored': 0.0, 'stormed': 0.0, 'story': 0.0, 'strain': 0.0, 'strained': 0.0, 'strains': 0.0, 'strategic': 0.0, 'strategists': 0.0, 'strategy': 0.0, 'streets': 0.0, 'strengthen': 0.0, 'strengthening': 0.0, 'strengths': 0.0, 'stressed': 0.0, 'strictly': 0.0, 'strike': 0.0, 'string': 0.0, 'stringent': 0.0, 'strong': 0.0, 'stronger': 0.0, 'struck': 0.0, 'structural': 0.0, 'students': 0.0, 'studied': 0.0, 'studies': 0.0, 'study': 0.0, 'style': 0.0, 'sub': 0.0, 'submarines': 0.0, 'subsequently': 0.0, 'substantial': 0.0, 'substations': 0.0, 'subversive': 0.0, 'success': 0.0, 'successful': 0.0, 'successfully': 0.0, 'successive': 0.0, 'succumb': 0.0, 'sudan': 0.0, 'sudden': 0.0, 'sued': 0.0, 'sufficient': 0.0, 'suggest': 0.0, 'suggested': 0.0, 'suggestion': 0.0, 'sugule': 0.0, 'summit': 0.0, 'sunday': 0.0, 'sundhara': 0.0, 'sunni': 0.0, 'superficial': 0.0, 'supermarket': 0.0, 'superpower': 0.0, 'superpowers': 0.0, 'supervision': 0.0, 'suppliers': 0.0, 'supply': 0.0, 'supplying': 0.0, 'support': 0.0, 'supported': 0.0, 'supporters': 0.0, 'supports': 0.0, 'supposed': 0.0, 'suriname': 0.0, 'surplus': 0.0, 'survey': 0.0, 'suspect': 0.035617535939241995, 'suspected': 0.0, 'suspects': 0.0, 'suspicions': 0.0, 'suspicious': 0.0, 'sustainable': 0.0, 'sustained': 0.0, 'sustaining': 0.0, 'suzhou': 0.0, 'sweden': 0.0, 'switch': 0.0, 'sympathetic': 0.0, 'synchronize': 0.0, 'syndicate': 0.0, 'system': 0.0, 'systematic': 0.0, 'systematically': 0.0, 'systems': 0.0, 'tactics': 0.0, 'tajikistan': 0.0, 'take': 0.0, 'taken': 0.035617535939241995, 'takeover': 0.0, 'takes': 0.0, 'taking': 0.0, 'taliban': 0.0, 'talks': 0.0, 'tanayev': 0.0, 'tang': 0.0, 'tank': 0.035617535939241995, 'tanks': 0.0, 'targeted': 0.0, 'targeting': 0.0, 'targets': 0.0, 'task': 0.0, 'tasks': 0.0, 'tat': 0.0, 'taught': 0.0, 'teaching': 0.0, 'teachings': 0.0, 'team': 0.0, 'teams': 0.0, 'tech': 0.0, 'technical': 0.0, 'technicians': 0.0, 'technological': 0.0, 'technologically': 0.0, 'technologies': 0.0, 'technology': 0.04242526098763271, 'teheran': 0.0, 'tehran': 0.0909287282669818, 'tel': 0.0, 'telephone': 0.0, 'television': 0.03262824797253317, 'temper': 0.0, 'temporary': 0.0, 'tension': 0.0, 'term': 0.0, 'terms': 0.0, 'territorial': 0.0, 'territory': 0.0, 'terrorism': 0.0, 'terrorist': 0.0, 'terrorists': 0.0, 'tertiary': 0.0, 'test': 0.0, 'th': 0.028415083216529174, 'thabo': 0.0, 'thailand': 0.0, 'theft': 0.0, 'thermal': 0.0, 'think': 0.035617535939241995, 'thinkers': 0.0, 'thinking': 0.0, 'third': 0.0, 'though': 0.0, 'thought': 0.0, 'threat': 0.0, 'threaten': 0.0, 'threatened': 0.0, 'threats': 0.0, 'throughout': 0.0, 'throw': 0.0, 'thus': 0.0, 'tier': 0.0, 'tiered': 0.0, 'ties': 0.0, 'tight': 0.0, 'tikapur': 0.0, 'time': 0.0, 'times': 0.0, 'timetable': 0.0, 'titov': 0.0, 'together': 0.0, 'tom': 0.035617535939241995, 'ton': 0.0, 'tonnage': 0.0, 'tons': 0.0, 'top': 0.0, 'total': 0.0, 'totaled': 0.0, 'totaling': 0.0, 'tougher': 0.0, 'tour': 0.0, 'toward': 0.0, 'towards': 0.0, 'town': 0.0, 'track': 0.0, 'tracking': 0.0, 'trade': 0.0, 'trades': 0.0, 'trading': 0.0, 'trafficker': 0.0, 'traffickers': 0.0, 'trafficking': 0.0, 'train': 0.0, 'trainers': 0.0, 'training': 0.0, 'tran': 0.0, 'tranquillity': 0.0, 'transfer': 0.0, 'transferred': 0.0, 'transfers': 0.0, 'transformation': 0.0, 'transmission': 0.0, 'transnational': 0.0, 'transparency': 0.035617535939241995, 'transport': 0.0, 'transportation': 0.0, 'transporting': 0.0, 'travel': 0.0, 'treatment': 0.0, 'treaty': 0.0, 'trend': 0.0, 'triad': 0.0, 'tricked': 0.0, 'triggered': 0.0, 'trilateral': 0.0, 'trip': 0.0, 'troops': 0.0, 'troubled': 0.0, 'troubling': 0.0, 'true': 0.0, 'trust': 0.0, 'trying': 0.0, 'tskhinvali': 0.0, 'tubing': 0.0, 'tug': 0.0, 'tupac': 0.0, 'turkistan': 0.0, 'turn': 0.0, 'turned': 0.0, 'twice': 0.0, 'two': 0.0, 'typical': 0.0, 'ueki': 0.0, 'ukraine': 0.0, 'ukrainian': 0.0, 'un': 0.0, 'unanimously': 0.03983070069524599, 'unannounced': 0.0, 'unarmed': 0.0, 'unauthorized': 0.0, 'unaware': 0.0, 'uncertainty': 0.0, 'unclear': 0.0, 'undeclared': 0.0, 'underground': 0.0, 'undermines': 0.0, 'undersecretary': 0.0, 'undertakes': 0.0, 'uneasiness': 0.0, 'unequal': 0.0, 'unilaterally': 0.0, 'union': 0.0, 'unique': 0.0, 'unit': 0.0, 'united': 0.0, 'units': 0.0, 'universities': 0.0, 'university': 0.0, 'unjustified': 0.03983070069524599, 'unknown': 0.0, 'unless': 0.0, 'unlikely': 0.0, 'unloaded': 0.0, 'unnamed': 0.0, 'unresolved': 0.0, 'unthreatened': 0.0, 'untraditional': 0.0, 'unusual': 0.0, 'update': 0.0, 'upgrade': 0.0, 'upgraded': 0.0, 'upon': 0.0, 'uprising': 0.0, 'urakov': 0.0, 'uranium': 0.06061915217798787, 'urban': 0.0, 'urged': 0.0, 'uruguay': 0.0, 'us': 0.0, 'use': 0.0, 'used': 0.05085159049964069, 'useful': 0.0, 'users': 0.0, 'uses': 0.0, 'using': 0.0, 'ustc': 0.0, 'utility': 0.0, 'utilization': 0.0, 'utter': 0.0, 'uzbekistan': 0.0, 'vaccines': 0.0, 'validated': 0.0, 'valley': 0.0, 'valves': 0.0, 'van': 0.0, 'variety': 0.0, 'various': 0.0, 'vehicle': 0.0, 'vehicles': 0.0, 'venezuela': 0.0, 'venezuelan': 0.0, 'venezuelans': 0.0, 'verifiably': 0.0, 'verification': 0.0, 'verified': 0.0, 'verify': 0.03262824797253317, 'verifying': 0.0, 'vessels': 0.0, 'veterinary': 0.0, 'veto': 0.0, 'victims': 0.0, 'video': 0.0, 'videotape': 0.0, 'viegas': 0.0, 'vienna': 0.0, 'vietnam': 0.0, 'vietnamese': 0.0, 'view': 0.0, 'views': 0.0, 'villepin': 0.0, 'viloria': 0.0, 'violate': 0.0, 'violates': 0.03983070069524599, 'violating': 0.0, 'violation': 0.0, 'violations': 0.0, 'violence': 0.0, 'violent': 0.0, 'violently': 0.0, 'virtual': 0.0, 'virus': 0.0, 'viruses': 0.0, 'visibility': 0.0, 'visit': 0.0, 'visited': 0.0, 'visiting': 0.0, 'visits': 0.0, 'vital': 0.0, 'vladimir': 0.0, 'vladimiro': 0.0, 'volatile': 0.0, 'volkswagen': 0.0, 'volume': 0.0, 'volunteered': 0.0, 'volunteers': 0.0, 'voted': 0.03983070069524599, 'wahhabi': 0.0, 'want': 0.0, 'wanted': 0.0, 'wants': 0.0, 'war': 0.0, 'warfare': 0.0, 'warhead': 0.0, 'warheads': 0.0, 'warn': 0.0, 'warned': 0.0, 'warning': 0.0, 'warships': 0.0, 'washington': 0.0909287282669818, 'watchdog': 0.0, 'watching': 0.0, 'water': 0.0, 'waterkloof': 0.0, 'waters': 0.0, 'waves': 0.0, 'way': 0.0, 'ways': 0.0, 'weaker': 0.0, 'weapon': 0.0, 'weaponry': 0.0, 'weapons': 0.0, 'web': 0.0, 'website': 0.0, 'week': 0.04423352147453158, 'weekend': 0.0, 'weekends': 0.0, 'weeks': 0.0, 'well': 0.0, 'welt': 0.0, 'wen': 0.0, 'west': 0.035617535939241995, 'western': 0.02211676073726579, 'whatever': 0.0, 'whether': 0.0, 'whole': 0.0, 'wide': 0.0, 'wider': 0.0, 'william': 0.0, 'willing': 0.0, 'willingness': 0.0, 'win': 0.0, 'windowpanes': 0.0, 'wine': 0.0, 'wing': 0.0, 'winning': 0.0, 'wired': 0.0, 'wishes': 0.0, 'withdraw': 0.0, 'within': 0.0, 'without': 0.0, 'woo': 0.0, 'words': 0.0, 'work': 0.030309576088993935, 'worked': 0.0, 'working': 0.0, 'works': 0.0, 'world': 0.0, 'worried': 0.0, 'worst': 0.0, 'worth': 0.0, 'would': 0.01889395861027712, 'wounded': 0.0, 'writers': 0.0, 'wrongdoing': 0.0, 'wrote': 0.03983070069524599, 'wu': 0.0, 'www': 0.0, 'xarardheere': 0.0, 'xinhua': 0.0, 'xp': 0.0, 'yaffa': 0.0, 'yam': 0.0, 'year': 0.0, 'years': 0.0, 'yellowcake': 0.0, 'yet': 0.0, 'yongbyon': 0.0, 'york': 0.0, 'young': 0.0, 'youngsters': 0.0, 'younis': 0.0, 'youtube': 0.0, 'zafaraniyah': 0.0, 'zilinskas': 0.0, 'zuma': 0.0}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"markdown","source":["### *SUPERT*"],"metadata":{"id":"GtUiH86CyARZ"}},{"cell_type":"code","source":["os.chdir('/content')\n","! git clone 'https://github.com/yg211/acl20-ref-free-eval.git'\n","os.chdir('/content/acl20-ref-free-eval')\n","! pip install -r requirements.txt\n","! pip uninstall scikit-learn -y\n","! pip install scikit-learn==0.23.1\n","clear_output()"],"metadata":{"id":"QdxxpDDQnMNZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Write stories and summaries to individual file for ease of use with SUPERT program\n","from os import path\n","from nltk.tokenize import sent_tokenize\n","os.chdir('/content/acl20-ref-free-eval')\n","\n","with open(f'{data_path}/snt_stories.txt', 'r') as file:\n","    stories = file.readlines()\n","    for idx, story in enumerate(stories):\n","        if path.exists(f'data/{dataset}/{idx}') == False:\n","            os.mkdir(f'data/{dataset}/{idx}')\n","            os.mkdir(f'data/{dataset}/{idx}/summaries')\n","            os.mkdir(f'data/{dataset}/{idx}/input_docs')\n","        with open(f'data/{dataset}/{idx}/input_docs/story.txt', 'w') as out:\n","            sentences = sent_tokenize(story)\n","            out.write('<P>' + '\\n')\n","            for sent in sentences:\n","                out.write(sent + '\\n')\n","                out.write('</P>' + '\\n')\n","                out.write('<P>' + '\\n')\n","\n","for idx, summary in enumerate(pred_summaries):\n","    with open(f'data/{dataset}/{idx}/summaries/summary.txt', 'w') as out:\n","        out.write(summary)"],"metadata":{"id":"dxxyuiQayLOe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ref_free_metrics.supert import Supert\n","from utils.data_reader import CorpusReader\n","\n","# read docs and summaries\n","# reader = CorpusReader(f'{data_path}/snt_summaries.txt')\n","for idx, _ in enumerate(pred_summaries):\n","    reader = CorpusReader(f'data/{dataset}/{idx}')\n","    source_docs = reader()\n","    summaries = reader.readSummaries() \n","\n","    # compute the Supert scores\n","    supert = Supert(source_docs, ref_metric='top15') \n","    supert_scores = supert(summaries)\n","    print(supert_scores)\n","# av_supert_score = sum(supert_scores)/len(supert_scores)\n","# print('SUPERT -> %5.3f' % (av_supert_score))"],"metadata":{"id":"vSnft6eqnxLF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### *Reference free scores (BLANC, SUPERT)*"],"metadata":{"id":"kpdvbs_p3KnK"}},{"cell_type":"code","source":["! pip install blanc\n","! pip install pytorch_transformers\n","from blanc import BlancHelp, BlancTune\n","blanc_help = BlancHelp()\n","clear_output()"],"metadata":{"id":"m2XXWjHyWfa8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(f'{data_path}/snt_stories.txt', 'r', encoding='latin-1') as file:\n","    stories = file.readlines()"],"metadata":{"id":"BKWiMo9Jms9i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["blanc_scores = blanc_help.eval_pairs(stories, pred_summaries)\n","av_blanc_score = sum(blanc_scores)/len(blanc_scores)\n","print('BLANC -> %5.3f' % (av_blanc_score))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Gsuk2Kgecok","executionInfo":{"status":"ok","timestamp":1659348345318,"user_tz":-60,"elapsed":483887,"user":{"displayName":"Max Greenwood","userId":"01363363807827242837"}},"outputId":"2b122465-4a7c-45d5-8c92-339b4bb673ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2700/2700 [08:05<00:00,  5.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["BLANC -> 0.090\n"]}]},{"cell_type":"code","source":["os.chdir('/content')\n","! git clone 'https://github.com/yg211/summary-reward-no-reference.git'\n","os.chdir('/content/summary-reward-no-reference')\n","clear_output()"],"metadata":{"id":"Mqgj6u23w8jK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from rewarder import Rewarder\n","rewarder = Rewarder(os.path.join('trained_models','sample.model'))\n","for idx, story in enumerate(stories):\n","    article = story\n","    summary = pred_summaries[idx]\n","    ref = ref_summaries[idx]\n","\n","    summ_score = rewarder(article, summary)\n","    ref_score = rewarder(article, ref)\n","    print(summ_score, ref_score)"],"metadata":{"id":"QPbzuWwHMevQ"},"execution_count":null,"outputs":[]}]}